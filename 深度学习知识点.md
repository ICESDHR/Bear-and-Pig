### 卷积神经网络(CNN)

- channel表示色彩通道

- 一个卷积神经网络主要由以下5种结构组成：

  - 输入层。
  - 卷积层。卷积层试图将神经网络中的每一块进行更加深入地分析从而得到抽象程度更高的特征。一般来说卷积层处理过得节点矩阵会变得更深。
  - 池化层。池化层神经网络不会改变三维矩阵的深度，但是它可以缩小矩阵的大小。

- 深度学习防止过拟合方法

  https://blog.csdn.net/NoviceRoad/article/details/75734048

  - 关于dropout

    https://blog.csdn.net/stdcoutzyx/article/details/49022443





### 如何训练

![](https://ws2.sinaimg.cn/large/006tKfTcly1ftqr8640etj30i00dtdiu.jpg)

####Choosing proper loss

- **when using softmax output layer,choose cross entropy**, why?

  ![FD588103-5631-464D-93F3-125A93D77FE0](https://ws1.sinaimg.cn/large/006tKfTcly1ftqr94zy4ej30il0bejvo.jpg)

#### Mini-batch

![20500BC0-39FB-435C-BBC9-BDAF4310E1F8](https://ws2.sinaimg.cn/large/006tKfTcly1ftqrg1t4ijj30gm096ab8.jpg)

#### New activation function

- 梯度消失问题（查资料补充）

  ![19997788-B3CF-4280-9F91-1242A6E0698A](https://ws3.sinaimg.cn/large/006tKfTcly1ftqsbuf72rj30ia0d3mzq.jpg)

**问**：使用ReLU作为激活函数的原因？

答：1. 计算快 2.Biological reason 3.Infinite sigmoid with different biases 4.为了解决梯度消失的问题

#### Adaptive Learning Rate





#### Momentum





### 过拟合

深度学习防止过拟合方法

https://blog.csdn.net/NoviceRoad/article/details/75734048

- 关于dropout

  https://blog.csdn.net/stdcoutzyx/article/details/49022443

#### 为什么会过拟合？

- 训练集和测试集不一致

  ![B8BFDDCA-824F-4DF4-B220-705A00A7587A](https://ws2.sinaimg.cn/large/006tKfTcly1ftru2s3yvoj30gd04wdg8.jpg)

#### 数据增强

- have more training data

- **create** more training data

  ![](https://ws1.sinaimg.cn/large/006tKfTcly1ftr1jzbqblj30lo06eq3r.jpg)



#### Early Stopping





#### Regularization

- Weight decay(权重衰减)



#### Dropout

dropout可以进一步提升模型可靠性并防止过拟合，dropou过程只在训练时使用。

dropout一般只在全连接层而不是卷积层或池化层使用

#### Network Structure





### 梯度弥散和梯度爆炸





### CNN

#### 入门讲解

https://blog.csdn.net/hjimce/article/details/47323463

https://blog.csdn.net/hjimce/article/details/51761865

直接看这两篇就行了！很好



#### 为什么选用CNN





####优化

dropout一般只在全连接层而不是卷积层或池化层使用



####调参





#### 了解哪些池化操作？

