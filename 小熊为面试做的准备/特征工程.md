## 特征工程

​	数据是信息的载体，但是原始的数据包含了大量的噪声，信息的表达也不够简练。因此，**特征工程的目的**，是通过一系列的工程活动，将这些信息**使用更高效的编码方式**（即特征）表示。使用特征表示的信息，信息损失较少，原始数据中包含的规律依然保留。此外，新的编码方式还需要**尽量减少原始数据中的不确定因素**（白噪声、异常数据、数据缺失…等等）**的影响**。 

- **特征工程包括：数据处理、特征选择、维度压缩**

![img](https://images2015.cnblogs.com/blog/967090/201701/967090-20170116151505067-1134887580.png) 

## 数据处理

异常点检测：

²   偏差检测：聚类、最近邻等

²   基于统计的异常点检测

例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。全距(Range)，又称极差，是用来表示统计资料中的变异量数(measures of variation) ，其最大值与最小值之间的差距；四分位距通常是用来构建箱形图，以及对概率分布的简要图表概述。

²   基于距离的异常点检测

主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。

²   基于密度的异常点检测

考察当前点周围密度，可以发现局部异常点，例如LOF算法



通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：

²  不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。

²  信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。

²  定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用独热编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。独热编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用独热编码后的特征可达到非线性的效果。

²  存在缺失值：缺失值需要补充。

²  信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征独热编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。

²  特征缺失：在本次的比赛中，影响人流量的因素还有天气、温度…等等，而这些因素在原始数据中并不存在，因此，需要在数据预处理的时候，将这些影响因素添加进来。

### 3.1无量纲化

　　无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。

### 3.1.1标准化

　　常用的方法是z-score标准化，经过处理后的数据均值为0，标准差为1，处理方法是：

​                                             x′=x−μδx′=x−μδ                                    （1）

公式一中，x’是标准化后的特征，x是原始特征值， 是样本均值， 是样本标准差。它们可以通过现有样本进行估计。在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。

#### 3.1.2 区间缩放法

常用的方法是通过对原始数据进行线性变换把数据映射到[0,1]之间，变换函数为：

​                                x′=x−minmax−minx′=x−minmax−min                           （2）

　　其中 min 是样本中最小值， max是样本中最大值，注意在数据流场景下最大值与最小值是变化的。另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。

#### 3.1.3归一化

简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为 的归一化公式如下：

​               x′=x∑mjx2j√x′=x∑jmxj2                               （3）

　　使用preproccessing库的Normalizer类对数据进行归一化的代码如下：

#### 3.2 对定量特征二值化（离散化）[1]

　　定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：

​            x′={10,,  x>thresholdx≤thresholdx′={1,  x>threshold0,x≤threshold                           （4）

#### 3.3 对定性特征进行独热编码

​       独热编码：使用一个二进制的位来表示某个定性特征的出现与否

#### 3.4 缺失值的处理

​    现实世界中的数据往往非常杂乱，未经处理的原始数据中某些属性数据缺失是经常出现的情况。另外，在做特征工程时经常会有些样本的某些特征无法求出。下面是几种处理数据中缺失值的主要方法。

##### 3.4.1. 删除

最简单的方法是删除，删除属性或者删除样本。如果大部分样本该属性都缺失，这个属性能提供的信息有限，可以选择放弃使用该维属性；如果一个样本大部分属性缺失，可以选择放弃该样本。虽然这种方法简单，但只适用于数据集中缺失较少的情况。

##### 3.4.2. 统计填充

对于缺失值的属性，尤其是数值类型的属性，根据所有样本关于这维属性的统计值对其进行填充，如使用平均数、中位数、众数、最大值、最小值等，具体选择哪种统计值需要具体问题具体分析。另外，如果有可用类别信息，还可以进行类内统计，比如身高，男性和女性的统计填充应该是不同的。

##### 3.4.3. 统一填充

对于含缺失值的属性，把所有缺失值统一填充为自定义值，如何选择自定义值也需要具体问题具体分析。当然，如果有可用类别信息，也可以为不同类别分别进行统一填充。常用的统一填充值有：“空”、“0”、“正无穷”、“负无穷”等。

##### 3.4.4 预测填充

我们可以通过预测模型利用不存在缺失值的属性来预测缺失值，也就是先用预测模型把数据填充后再做进一步的工作，如统计、学习等。虽然这种方法比较复杂，但是最后得到的结果比较好。

##### 3.4.5具体分析

上面两次提到具体问题具体分析，为什么要具体问题具体分析呢？因为属性缺失有时并不意味着数据缺失，缺失本身是包含信息的，所以需要根据不同应用场景下缺失值可能包含的信息进行合理填充。下面通过一些例子来说明如何具体问题具体分析，仁者见仁智者见智，仅供参考：

1. “年收入”：商品推荐场景下填充平均值，借贷额度场景下填充最小值；
2. “行为时间点”：填充众数；
3. “价格”：商品推荐场景下填充最小值，商品匹配场景下填充平均值；
4. “人体寿命”：保险费用估计场景下填充最大值，人口估计场景下填充平均值；
5. “驾龄”：没有填写这一项的用户可能是没有车，为它填充为0较为合理；
6. ”本科毕业时间”：没有填写这一项的用户可能是没有上大学，为它填充正无穷比较合理；
7. “婚姻状态”：没有填写这一项的用户可能对自己的隐私比较敏感，应单独设为一个分类，如已婚1、未婚0、未填-1。 

#### 3.5 数据变换

　　常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：

(x′1,x′2...x′15)=(1,x1,x2,x3,x4,x21,x1∗x2,x1∗x3,x1∗x4,x22,x2∗x3,x2∗x4,x23,x3∗x4,x24)(x1′,x2′...x15′)=(1,x1,x2,x3,x4,x12,x1∗x2,x1∗x3,x1∗x4,x22,x2∗x3,x2∗x4,x32,x3∗x4,x42)  （5）



**多项式特征变换，目标是将特征两两组合起来，使得特征和目标变量之间的的关系更接近线性，从而提高预测的效果**

## 特征选择

当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。

**目的是**

- 简化模型，增加模型的可解释性
- 缩短训练时间
- 避免维度灾难
- 改善模型通用性、降低过拟合

通常来说，从两个方面考虑来选择特征：

- **特征是否发散**：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- **特征与目标的相关性**：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。

　　根据特征选择的形式又可以将特征选择方法分为3种：

- **Filter：过滤法**，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- **Wrapper：包装法**，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- **Embedded：嵌入法**，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

### 1 Filter过滤法

#### 1.1 方差选择法

　　使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。

#### 1.2 相关系数法

　　使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。 

#### 1.3 卡方检验

　　经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：

 x2=∑(A−E)2Ex2=∑(A−E)2E      （6）

　　这个统计量的含义简而言之就是自变量对因变量的相关性。选择卡方值排在前面的K个特征作为最终的特征选择

#### 1.4 互信息法

　　经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：

 I(X:Y)=∑x∈X∑y∈Yp(x,y)logp(x,y)p(x)p(y)I(X:Y)=∑x∈X∑y∈Yp(x,y)logp(x,y)p(x)p(y)   （7）

　　同理，选择互信息排列靠前的特征作为最终的选取特征

### 2 Wrapper包装法

#### 2.1 递归特征消除法

　　递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

### 3 Embedded嵌入法

#### 3.1 基于惩罚项的特征选择法

　　使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。由于L1L1范数有筛选特征的作用，因此，训练的过程中，如果使用了L1L1范数作为惩罚项，可以起到特征筛选的效果

#### 3.2 基于树模型的特征选择法

　　训练能够对特征打分的预选模型：GBDT、随机森林RandomForest和逻辑回归Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；

​     GBDT: http://breezedeus.github.io/2014/11/19/breezedeus-feature-mining-gbdt.html

### 4 特征组合

​	通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见

## 维度压缩

目的：最大限度地降低数据的维度的前提下能够同时保证保留目标的重要的信息 

![](C:\Users\admin\Desktop\Job\为面试做准备\笔记里的图片\特征降维.jpg)

## 3.2 主成分分析/PCA

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7nsiw5j30k00b9t9c.jpg)

 

![img](https://ws3.sinaimg.cn/large/006tKfTcgy1fm7z80jfrmj30k00b9aaz.jpg)

 

![img](https://ws3.sinaimg.cn/large/006tKfTcgy1fm7z7vtyksj30k00b9q3w.jpg)

## 3.3 线性判别分析/LDA

![img](https://ws1.sinaimg.cn/large/006tKfTcgy1fm7z7ki2i4j30k00b9aad.jpg)

 

![img](https://ws2.sinaimg.cn/large/006tKfTcgy1fm7z7qoar0j30k00b9mxk.jpg)

 

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7thaduj30k00b9t9p.jpg)

 

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7wdce9j30k00b9jsf.jpg)

 

![img](https://ws1.sinaimg.cn/large/006tKfTcgy1fm7z7s2wclj30k00b9js8.jpg)

## 3.4局部线性嵌入/LLE

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7m2j3ej30k00b9t9m.jpg)

 

![img](https://ws1.sinaimg.cn/large/006tKfTcgy1fm7z6fxspcj30k00b9myn.jpg)

 

![img](https://ws2.sinaimg.cn/large/006tKfTcgy1fm7z7r5zu8j30k00b93z7.jpg)

 

![img](https://ws2.sinaimg.cn/large/006tKfTcgy1fm7z7rn4egj30k00b9mxr.jpg)

 

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7txdlrj30k00b9mxp.jpg)

## 3.5拉普拉斯特征映射/LE

![img](https://ws2.sinaimg.cn/large/006tKfTcgy1fm7z7ve2dej30k00b90t7.jpg)

 

![img](https://ws1.sinaimg.cn/large/006tKfTcgy1fm7z8021wmj30k00b9jrw.jpg)

 

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7psl90j30k00b9aaj.jpg)

 

![img](https://ws2.sinaimg.cn/large/006tKfTcgy1fm7z7y8f64j30k00b9q3v.jpg)

## 3.6 随机邻域嵌入/SNE

![img](https://ws3.sinaimg.cn/large/006tKfTcgy1fm7z7q8uctj30k00b9dgk.jpg)

 

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7oqvf8j30k00b9jsb.jpg)

## 3.7 t-分布邻域嵌入/T-SNE

![img](https://ws4.sinaimg.cn/large/006tKfTcgy1fm7z7o99dbj30k00b90t9.jpg)

 

![img](https://ws2.sinaimg.cn/large/006tKfTcgy1fm7z7neejgj30k00b9gmn.jpg)

 

![img](https://ws3.sinaimg.cn/large/006tKfTcgy1fm7z7sgjmij30k00b9gmf.jpg)