### 一、逻辑回归（会推导）

<https://blog.csdn.net/chibangyuxun/article/details/53148005>

![](C:\Users\admin\Desktop\Job\为面试做准备\笔记里的图片\逻辑回归.png)

### 二、正则项（公式后半部分）

<https://blog.csdn.net/zouxy09/article/details/24971995>

1）L0范数是指向量中非0的元素的个数

​     L1范数是指向量中各个元素绝对值之和（特征选择）

​     L2范数是指向量各元素的平方和然后求平方根（过拟合）

### 三、损失函数（公式前半部分）

<https://blog.csdn.net/shenxiaoming77/article/details/51614601>

### 四、支持向量机

1）Hinge-Loss:<https://zhuanlan.zhihu.com/p/35708936>

2) 了解的不多以后再看吧

### 五、boosting, bagging, stacking的相同及不同之处

<https://blog.csdn.net/Mr_tyting/article/details/72957853>

<https://blog.csdn.net/sinat_26917383/article/details/54667077>

### 六、Adaboost, gbdt, xgboost，Random Forests

<https://www.cnblogs.com/willnote/p/6801496.html>

<https://blog.csdn.net/qq_28031525/article/details/70207918>

<https://www.zhihu.com/question/41354392>

三种方法的损失函数不同

#### 1）Adaboost

用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。

#### 2) gbdt

GBDT通过负梯度来识别问题，通过计算负梯度来改进模型，即每一轮产生的残差作为下一轮回归树的输入，下一轮的回归树的目的就是尽可能的拟合这个输入残差。

a. GBDT算法自动构造新特征

<http://baijiahao.baidu.com/s?id=1580921142553585711&wfr=spider&for=pc>

#### 3) xgboost

（xgboost的介绍、使用情况、调参数、如何做特征选择、如何解决过拟合情况）

<http://x-algo.cn/index.php/2016/07/24/xgboost-principle/>

<https://www.zhihu.com/answer/98658997>

GBDT算法只利用了一阶的导数信息，xgboost对损失函数做了二阶的泰勒展开，并在目标函数之外加入了正则项对整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合。XGBoost是通过上式的损失函数下降程度来判断。

### 七、标准梯度下降和随机梯度下降

<https://blog.csdn.net/ortyijing/article/details/54984058>

### 八、K近邻 KNN

K近邻+Kd树

1）kd树选择每个维度上方差最大的维度进行分割，原因是特征的方差越大，沿该坐标轴方向上的数据点就越分散，在这个方向上进行数据分割就可以获得最好的分辨率。

<https://blog.csdn.net/app_12062011/article/details/51986805>

### 九、聚类

<https://www.zhihu.com/question/34554321>(第三个回答比较好)

1）原型聚类（k-means）

2）密度聚类（DBSCAN）

3）层次聚类（AGNES）

### 十、梯度下降，牛顿法和拟牛顿法

<https://www.zhihu.com/question/19723347>

<https://www.cnblogs.com/shixiangwan/p/7532830.html> 

### 十一、不平衡样本集处理方法（西瓜书P66）

#### 1）为什么要处理不平衡样本集？

A. 不平衡样本集的场景出现在很多实际应用场景中

B. 训练出的模型在多数类和少数类上表现差别大

C. 不同分类的误判带来的影响不同

#### 2）如何处理不平衡样本集？

A. 改变数据分布

a.随机采样法

​	随机过采样（增加少数类样本个数）--模型过拟合

​	随机欠采样（减少多数类样本个数）--信息缺失

b.过采样的代表算法SMOTE

​	对训练集中的少数类样本进行插值来产生额外的该类样本，而非单纯的重复采样

c.欠采样的代表算法EasyEnsemble

​	利用集成学习机制，将多数类划分为若干个集合供不同学习器使用。

B. 改变算法本身

​	再缩放原理（阈值移动）

### 十二、评判标准

<https://blog.csdn.net/sinat_26917383/article/details/75199996?locationNum=3&fps=1>

ROC和AUC

<https://www.zhihu.com/question/39840928?from=profile_question_card>

AUC是指 随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值 比 分类器输出该负样本为正的那个概率值 要大的可能性。

### 十三、常用的数据清洗方法

1）缺失值处理

2）异常值检测

3）数据规约（属性选择，数据采样）

### 十四、数据降维

<https://blog.csdn.net/julialove102123/article/details/79952721>

#### 1）降维原因

密采样：即使很小的范围内仍然可以找到很多样本

维度灾难：高维情况下出现数据样本稀疏，距离计算困难

### 2）降维方法

a. PCA（最大化保持数据信息）

​	<https://blog.csdn.net/google19890102/article/details/27969459>

​	<https://www.zhihu.com/question/20507061>

​	最近重构性：样本点到这个超平面的距离都足够近；

​	最大可分性：样本点在这个超平面上的投影能尽可能分开。

b. LDA（使降维后的数据点尽可能容易区分）(西瓜书P60)

​	同类样例投影点的协方差尽可能小，异类样例投影点中心尽可能远。

c.低维嵌入

​	MDS算法：降维后的距离与原始空间中的距离尽可能接近。

### 十五、深度神经网络的重要使用模型

#### 1.CNN网络

卷积层：主要目的是使原信号特征增强，并且降低噪声。

池化层：主要是降低网络训练参数及模型的过拟合程度。

激活层：加入非线性因素。

1）卷积在图像中的作用

模糊与锐化

边缘提取

可以理解为观察图片的方式

2）卷积在CNN中的作用

稀疏链接，减少参数

局部探测

参数共享

3）避免过拟合的方法Dropout

训练时，随机丢弃网络中的一些节点，非永久删除，仅仅在一次训练中不参与，每个 batch 训练中丢弃的节点都重新随机选择。

#### LeNet：

输入层：32*32

卷积层：3个（5*5）

降采样层：2个（平均池化）

全连接层：1个

输出层：高斯连接：10个类别（数字0-9的概率）

![](C:\Users\admin\Desktop\Job\为面试做准备\笔记里的图片\LeNet.png)

#### AlexNet：

<https://wenku.baidu.com/view/f66123e60b4c2e3f5627634b.html>

卷积层：5个

池化层：5个

全连接层：3个

softmax：

重叠池化：

![](C:\Users\admin\Desktop\Job\为面试做准备\笔记里的图片\AlexNet.png)

1) 引入多种防止过拟合的技巧：Dropout，数据增强

a.数据增强：通过一些变换从已有的训练数据集中生成一些数据，扩大训练数据量

水平图像翻转

给图像增加一些随机的光照（色彩变幻，颜色抖动）

从原始图像随机的平移转换（改变大小）

AlexNet使用的是：对训练数据及西宁左右对称以及平行变换，将训练数据增加为原来的2048倍，并且对像素机型PCA变换构造新的样本。

b. Dropout：概率0.5

2) 非线性激活函数：ReLU

使用ReLU代替Sigmoid激活函数，发现得到的SGD的收敛速度比sigmoid/tanh快很多

大数据训练：120万

3) 两个GPU：

一个GPU运行图中顶部的层次部分，一个GPU运行图中底部的层次部分。

4) LRN（local response normalization局部归一化）：平滑处理，防止激活函数饱和

<https://blog.csdn.net/hduxiejun/article/details/70570086>

#### VGG 是更深的 AlexNet（19层）:

改进：

小尺寸卷积核叠加：减少训练的参数，减轻过拟合，增加网络表达力

在整个图片和多尺度（multi-scale)上训练和测试图片,多模型融合

采用最大池化

#### GoogleNet（22层）: NIN模型：

<https://zhuanlan.zhihu.com/p/31199400>

对传统的卷积方法做出了改进：

多个1*1的卷积核级联就可以实现对多通道的feature map做非线性的组合，再配合激活函数，就可以实现MLP结构。

全局平均池化：代替原来的全连接层

多特征结合：1*1，3*3，5*5的卷积核做卷积之后再拼接起来。

优点：

更好的局部抽象能力

更小的参数空间

更小的全局over-fitting

#### ResNet（152-1000层）：

<https://www.zhihu.com/question/64494691>

残差的含义：如果能使用基层网络去逼近一个复杂的非线性映射H(x)来预测图片，则同样可以用这几层网络去逼近他的残差函数F(x) = H(x) - x,并且更加简单。

![](C:\Users\admin\Desktop\Job\为面试做准备\笔记里的图片\ResNet.png)

  			两层的3×3卷积                                                        三层的借鉴NIN和Inception Net

​                                                                                  			  三层的在3×3前后有个升降维

特点：

网络较瘦，控制参数数量

存在明显层级，特征图逐层增加

无dropout，利用BN和全局平均池化进行正则化（Tensorflow书上有解释）

高层减少3×3卷积个数，用1×1控制特征图数量，“瓶颈”(bottleneck)

批标准化（BN）的作用：

客服神经网络层数加深导致难以训练的问题

固定每一层输入信号的方差及均值，保持稳定的数据分布

批标准化（BN）的优点：

加大探索的步长，加快收敛速度

更容易跳出局部最小

破坏原来的数据分布，一定程度上缓解过拟合

### 十六、排序算法

#### 时间复杂度O（n^2）:

冒泡排序：   空间复杂度O（1）

选择排序：   空间复杂度O（1）

插入排序：   空间复杂度O（1）

#### 时间复杂度为O（nlogn）

归并排序：   空间复杂度O（n）

快速排序:     空间复杂度O（lgn）-O(n)

堆排序：      空间复杂度O（1）

希尔排序：  空间复杂度O（1）

#### 时间复杂度为O（n）

计数排序：  空间复杂度O（m）

基数排序：  空间复杂度O（m）根据个位，十位分别排序

#### 排序算法的稳定性：

稳定的：冒泡排序，插入排序，归并排序，计数排序，基数排序，桶排

不稳定的：选择排序，快速排序，希尔排序，堆排序

#### 小技巧：

1）已知一个几乎有序的数组，如果把数组排好序，每个元素移动的距离不超过k，如何排序？

改进后的堆排序，前k个建立最小堆，每次弹出最小的。O(n*logK）

2）判断数组中是否有重复值，要求空间复杂度为O(1)？

非递归的堆排序

### 十七、笔试题

#### 1）字符串相关

回文，子串，子序列，大整数计算，字符计数，滑动窗口，寻找无重复字符子串问题，动态规划（最长子串，最长回文。。）

#### 2）链表相关

回文结构（栈，栈和快慢指针）

判断是否有环，返回环的第一个结点（快慢指针先走一遍，在某处相遇，然后快指针再从头一次一步和慢指针一起，然后再相遇就是第一个结点）

判断两个无环单链表是否相交（让长链表先走差值步，然后两个链表再一起走）

#### 3）二叉树相关

<http://baijiahao.baidu.com/s?id=1579863346584939023&wfr=spider&for=pc>

子树，平衡二叉树，搜索二叉树，完全二叉树，后继结点和前驱结点（中序遍历）

#### 4）布隆过滤器

#### 5）位运算

#### 6）概率题（少）

#### 7）大数据问题（Map Reduce和Hadoop逐渐成为热门）

分而治之，常用hashmap及bitmap

#### 8）动态规划，分治法

### 十八、TCP/IP三次握手，四次挥手