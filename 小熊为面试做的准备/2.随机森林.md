​	随机森林（Random Forest）是属于集成学习的一种组合分类算法（确切说是属于bagging），集成学习的核心思想就是将若干个弱（基）分类器组合起来，得到一个分类性能显著优越的强分类器。如果各弱分类器之前没有强依赖关系、可并行生成，就可以使用随机森林算法。 

  随机森林利用自主抽样法（bootstrap）从原数据集中有放回地抽取多个样本，对抽取的样本先用弱分类器—决策树进行训练，然后把这些决策树组合在一起，通过投票得出最终的分类或预测结果。

#### 随机森林的生成方法

1. 从样本集中通过重采样的方式产生n个样本。
2. 建设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点。
3. 重复m次，产生m棵决策树。
4. 多数投票机制进行预测。

#### 随机森林中的随机是什么意思？

​     随机森林中的随机性主要体现在两个方面：

1. **随机采样**：随机森林在计算每棵树时，从全部训练样本（样本数为n）中选取一个可能有重复的、大小同样为n的数据集进行训练（即booststrap采样）。
2. **特征选取的随机性**：在每个节点随机选取所有特征的一个子集，用来计算最佳分割方式。

#### 随机森林的优点：

1. 表现性能好，与其他算法相比有着很大优势。
2. 随机森林能处理很高维度的数据（也就是很多特征的数据），并且不用做特征选择。
3. 在训练完之后，随机森林能给出哪些特征比较重要。
4. 训练速度快，容易做成并行化方法(训练时，树与树之间是相互独立的)。
5. 在训练过程中，能够检测到feature之间的影响。
6. 对于不平衡数据集来说，随机森林可以平衡误差。当存在分类不平衡的情况时，随机森林能提供平衡数据集误差的有效方法。
7. 如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。
8. 随机森林算法有很强的抗干扰能力（具体体现在6,7点）。所以当数据存在大量的数据缺失，用RF也是不错的。
9. **随机森林抗过拟合能力比较强**（虽然理论上说随机森林不会产生过拟合现象，但是在现实中噪声是不能忽略的，增加树虽然能够减小过拟合，但没有办法完全消除过拟合，无论怎么增加树都不行，再说树的数目也不可能无限增加的。）
10. 随机森林能够解决分类与回归两种类型的问题，并在这两方面都有相当好的估计表现。（虽然RF能做回归问题，但通常都用RF来解决分类问题）。
11. 在创建随机森林时候，对generlization error(泛化误差)使用的是无偏估计模型，泛化能力强。

#### 随机森林的缺点：

1. 随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。
2. 对于许多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试。
3. 可能有很多相似的决策树，掩盖了真实的结果。
4. 对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。
5. 执行数据虽然比boosting等快（随机森林属于bagging），但比单只决策树慢多了。

**PS:最后几个重要的点**

1. **RF采用多个决策树的投票机制来改善决策树。**
2. **为什么不能用全样本去训练m棵决策树？**

​        答：全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的（如果有m个决策树，那就需要m个一定数量的样本集来训练每一棵树）

​     3.**产生n个样本的方法，采用Bootstraping法，这是一种又放回的抽样方法，产生n个样本。**

​     4.**最终采用Bagging的策略来获得，即多数投票机制。**



#### **bagging**与**Boosting**算法

看起来相似，但是基分类器的训练方法完全不同，区别为：

Bagging算法的训练集往往是从原数据集中有放回的抽样得到的（**原数据集的一部分**），每个基分类器是**相互独立**的，**并列**的。因为每个基分类器训练方法独立且相同，所以最后分类器**等权重**投票。

而在boosting算法中，基分类器是**依次训练**的，因为分错的点在接下来的训练时会更加的**被侧重**，也就是说，**每个基分类器的训练都是建立在之前基分类器的表现基础之上**的。最后分类器加权投票。



####  **boosting**

依次训练k个子分类器，最终的分类结果由这些子分类器投票决定。首先从大小为n的原始训练数据集中随机选取n1n1个样本训练出第一个分类器，记为C1C1，然后构造第二个分类器C2C2的训练集D2D2，要求：D2D2中一半样本能被C1C1正确分类，而另一半样本被C1C1错分。接着继续构造第三个分类器C3C3的训练集D3D3，要求：C1C1、C2C2对D3D3中样本的分类结果不同。剩余的子分类器按照类似的思路进行训练。 

早期的Boosting算法存在缺陷，即需要事先知道弱学习算法的分类正确的下限，并不是自适应的，这限制了该算法在现实中的应用。这一缺陷在之后的adaptive boosting（AdaBoost）算法中被解决。 

#### AdaBoost具体做法举例：

​	对于一个数据集，先用任一弱分类算法训练得到一弱分类器，根据对训练集分对分错情况，对训练集样本分配权重，分错的样本权重更高。根据这个权重对训练集进行挑选得到新的训练集，权重大的样本更可能被选到，以此来侧重对于之前分错的样本的训练，得到第二个分类器。以此类推，不断训练多个基分类器，最后根据各个基分类器的准确率赋予分类器权重。当需要判别时，加权投票得最终判断结果。

随机森林、GBDT、XGboost

https://blog.csdn.net/qq_28031525/article/details/70207918