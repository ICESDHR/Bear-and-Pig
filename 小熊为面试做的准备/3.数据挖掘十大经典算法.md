##  **一、 C4.5** 

C4.5算法是机器学习算法中的一种分类决策树算法,其核心算法是ID3 算法.   C4.5算法继承了ID3算法的长处。并在下面几方面对ID3算法进行了改进： 
1) 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足。 
2) 在树构造过程中进行剪枝； 
3) 可以完毕对连续属性的离散化处理； 
4) 可以对不完整数据进行处理。 
C4.5算法有例如以下长处：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，须要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。

1、机器学习中。决策树是一个预測模型。他代表的是对象属性与对象值之间的一种映射关系。树中每一个节点表示某个对象，而每一个分叉路径则代表的某个可能的属性值，而每一个叶结点则
相应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出。若欲有复数输出，能够建立独立的决策树以处理不同输出。

2、 从数据产生决策树的机器学习技术叫做决策树学习,  通俗说就是决策树。 
3、决策树学习也是数据挖掘中一个普通的方法。在这里，每一个决策树都表述了一种树型结构，他由他的分支来对该类型的对象依靠属性进行分类。每一个决策树能够依靠对源数据库的切割
进行数据測试。

这个过程能够递归式的对树进行修剪。

当不能再进行切割或一个单独的类能够被应用于某一分支时。递归过程就完毕了。

另外。随机森林分类器将很多决策树结合起来
以提升分类的正确率。

决策树是怎样工作的？ 
1、决策树一般都是自上而下的来生成的。

2、选择切割的方法有好几种，可是目的都是一致的：对目标类尝试进行最佳的切割。

3、从根到叶子节点都有一条路径，这条路径就是一条―规则 
4、决策树能够是二叉的，也能够是多叉的。 
对每一个节点的衡量： 
1)         通过该节点的记录数 
2)         假设是叶子节点的话，分类的路径 
3)         对叶子节点正确分类的比例。 
有些规则的效果能够比其它的一些规则要好。

因为ID3算法在实际应用中存在一些问题。于是Quilan提出了C4.5算法，严格上说C4.5仅仅能是ID3的一个改进算法。相信大家对ID3算法都非常.熟悉了，这里就不做介绍。 
C4.5算法继承了ID3算法的长处，并在下面几方面对ID3算法进行了改进： 
1) 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
2) 在树构造过程中进行剪枝。 
3) 可以完毕对连续属性的离散化处理； 
4) 可以对不完整数据进行处理。 
C4.5算法有例如以下长处：产生的分类规则易于理解，准确率较高。

其缺点是：在构造树的过程中。须要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。

此外，C4.5仅仅适合于
可以驻留于内存的数据集。当训练集大得无法在内存容纳时程序无法执行。  来自搜索的其它内容： 
 C4.5算法是机器学习算法中的一种分类决策树算法,其核心算法是ID3算法.  分类决策树算法是从大量事例中进行提取分类规则的自上而下的决策树. 决策树的各部分是: 
             根:    学习的事例集. 
             枝:    分类的判定条件. 
             叶:    分好的各个类. 

###  **ID3算法** 

   1.概念提取算法CLS 
1)      初始化參数C={E},E包含全部的样例,为根. 
2)        IF      C中的任一元素e同属于同一个决策类则创建一个叶子     
               节点YES终止. 
           ELSE      依启示式标准,选择特征Fi={V1,V2,V3,．．．Vn}并创建 
                       判定节点 
划分C为互不相交的N个集合C1,C2,C3,．．．,Cn。 
3)      对任一个Ci递归. 
    2.      ID3算法 
1)      随机选择C的一个子集W    (窗体). 
2)      调用CLS生成W的分类树DT(强调的启示式标准在后). 
3)      顺序扫描C搜集DT的意外(即由DT无法确定的样例). 
4)      组合W与已发现的意外,形成新的W. 
5)      反复2)到4),直到无例外为止. 
启示式标准: 
       仅仅跟本身与其子树有关,採取信息理论用熵来量度. 
       熵是选择事件时选择自由度的量度,其计算方法为 
               P    =    freq(Cj,S)/|S|; 
       INFO(S)=    -    SUM(    P*LOG(P)    )    ;        SUM()函数是求j 从1到n和. 
       Gain(X)=Info(X)-Infox(X); 
       Infox(X)=SUM(    (|Ti|/|T|)*Info(X); 
为保证生成的决策树最小,ID3 算法在生成子树时,选取使生成的子树的熵(即Gain(S))最小的
的特征来生成子树. 
  3、  ID3算法对数据的要求 
1）.      全部属性必须为离散量. 
2）.      全部的训练例的全部属性必须有一个明白的值. 
3）.      同样的因素必须得到同样的结论且训练例必须唯一. 

###    **C4.5对ID3算法的改进:** 

​       1.      熵的改进,加上了子树的信息. 
             Split_Infox(X)=    -    SUM(      (|T|/|Ti|    )    *LOG(|Ti|/|T|)      ); 
             Gain    ratio(X)=      Gain(X)/Split    Infox(X); 
        2.      在输入数据上的改进. 
         1) 
因素属性的值能够是连续量,C4.5 对其排序并分成不同的集合后依照ID3 算法当作离散量进行处理,但结论属性的值必须是离散值. 
       2)    训练例的因素属性值能够是不确定的,以    ?    表示,但结论必须是确定的 

       3.      对已生成的决策树进行裁剪,减小生成树的规模.



## **二、k-means** 

术语“k-means”最早是由James MacQueen在1967年提出的。这一观点能够追溯到1957年 Hugo Steinhaus所提出的想法。1957年。斯图亚特·劳埃德最先提出这一标准算法，当初是作为一门应用于脉码调制的技术,直到1982年，这一算法才在贝尔实验室被正式提出。1965年。 E.W.Forgy发表了一个本质上是同样的方法。1975年和1979年。Hartigan和Wong分别提出了一个更高效的版本号。

算法描写叙述
输入：簇的数目k；包括n个对象的数据集D。

输出：k个簇的集合。

方法：
从D中随意选择k个对象作为初始簇中心；
repeat;
依据簇中对象的均值。将每一个对象指派到最相似的簇；
更新簇均值。即计算每一个簇中对象的均值；
计算准则函数；
until准则函数不再发生变化。

算法的性能分析
   1）长处
（1）k-平均算法是解决聚类问题的一种经典算法，算法简单、高速。
（2）对处理大数据集，该算法是相对可伸缩的和高效率的。由于它的复杂度大约是O（nkt），当中n是全部对象的数目，k是簇的数目,t是迭代的次数。

通常k<<n。这个算法常常以局部最优结束。
（3）算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，而簇与簇之间差别明显时，它的聚类效果非常好。
   2）缺点
（1）k-平均方法仅仅有在簇的平均值被定义的情况下才干使用。不适用于某些应用。如涉及有分类属性的数据不适用。
（2）要求用户必须事先给出要生成的簇的数目k。

（3）对初值敏感。对于不同的初始值。可能会导致不同的聚类结果。

（4）不适合于发现非凸面形状的簇，或者大小区别非常大的簇。
（5）对于"噪声"和孤立点数据敏感，少量的该类数据可以对平均值产生极大影响。
算法的改进
针对算法存在的问题，对K-means算法提出一些改进：
一是数据预处理，
二是初始聚类中心选择。
三是迭代过程中聚类种子的选择。
1、首先对样本数据进行正规化处理，这样就能防止某些大值属性的数据左右样本间的距离。给定一组含有n个数据的数据集，每个数据含有m个属性。分别计算每个属性的均值、标准差对每条数据进行标准化。

3、其次，初始聚类中心的选择对最后的聚类效果有非常大的影响，原K-means算法是随机选取k个数据作为聚类中心，而聚类的结果要是同类间尽可能相似，不同类间尽可能相异，所以初始聚类中心的选取要尽可能做到这一点。

採用基于距离和的孤立点定义来进行孤立点的预先筛选，并利用两两数据之间的最大距离在剩余数据集合中寻找初始聚类中心。但对于实际数据。孤立点个数往往不可预知。在选择初始聚类中心时，先将孤立点纳入统计范围。在样本中计算对象两两之间的距离，选出距离最大的两个点作为两个不同类的聚类中心，接着从其余的样本对象中找出已经选出来的全部聚类中心的距离和最大的点为还有一个聚类中心，直到选出k个聚类中心。

这样做就减少了样本输入顺序对初始聚类中心选择的影响。
聚类中心选好以后，就要进行不断的迭代计算，在K-means算法中。是将聚类均值点(类中全部数据的几何中心点)作为新的聚类种子进行新一轮的聚类计算，在这样的情况下。新的聚类种子可能偏离真正的数据密集区，从而导致偏差，特别是在有孤立点存在的情况下，有非常大的局限性。在选择初始中心点时，由于将孤立点计算在内，所以在迭代过程中要避免孤立点的影响。

这里依据聚类种子的计算时，採用簇中那些与第k-1轮聚类种子相似度较大的数据，计算他们的均值点作为第k轮聚类的种子，相当于将孤立点排除在外。孤立点不參与聚类中心的计算。这样聚类中心就不会由于孤立点的原因而明显偏离数据集中的地方。在计算聚类中心的时候。要运用一定的算法将孤立点排除在计算均值点那些数据之外，这里主要採用类中与聚类种子相似度大于某一阈值的数据组成每一个类的一个子集。计算子集中的均值点作为下一轮聚类的聚类种子。

为了能让很多其它的数据參与到聚类中心的计算种去，阈值范围要包括大多数的数据。

在第k-1轮聚类获得的类。计算该类中全部数据与该类聚类中心的平均距离S,选择类中与聚类种子相似度大于2S的数据组成每一个类的一个子集，以此子集的均值点作为第k轮聚类的聚类种子。在数据集中不管是否有明显的孤立点存在。两倍的平均距离都能包括大多数的数据。
对孤立点的改进—基于距离法
经典k均值算法中没有考虑孤立点。所谓孤立点都是基于距离的, 是数据U集中到U中近期邻居的距离最大的对象, 换言之, 数据集中与其近期邻居的平均距离最大的对象。针对经典k均值算法易受孤立点的影响这一问题, 基于距离法移除孤立点, 详细步骤例如以下:
首先扫描一次数据集, 计算每个数据对象与其临近对象的距离, 累加求其距离和, 并计算出距离和均值。假设某个数据对象的距离和大于距离和均值, 则视该点为孤立点。把这个对象从数据集中移除到孤立点集合中, 反复直到全部孤立点都找到。

最后得到新的数据集就是聚类的初始集合。

对随机选取初始聚类中心的改进
经典k均值算法随机选取k个点作为初始聚类中心进行操作。因为是随机选取, 则变化较大, 初始点选取不同, 获得聚类的结果也不同。而且聚类分析得到的聚类的准确率也不一样。

对k均值算法的初始聚类中心选择方法—随机法进行改进, 其根据是聚类过程中同样聚类中的对象是相似的, 相异聚类中的对象是不相似的。

因此提出了一种基于数据对象两两间的距离来动态寻找并确定初始聚类中心的思路, 详细步骤例如以下:
首先整理移除孤立点后的数据集U,记录数据个数y,令m=1。比較数据集中全部数据对象两两之间的距离。找出距离近期的2个数据对象形成集合Am;比較Am中每个数据对象与数据对象集合U中每个对象的距离,在U中找出与Am 中近期的数据对象,优先吸收到Am 中,直到Am 中的数据对象个数到达一定数值,然后令m=m+1。

再从U中找到对象两两间距离近期的2个数据对象构成Am,反复上面的过程,直到形成k个对象集合。这些集合内部的数据是相似的,而集合间是相异的。

能够看出,这样的聚类方法同一时候满足下面2个条件:①每一个组至少包括一个数据对象; ②每一个数据对象必须属于且仅属于一个组。

即数据对象Xi ∈Ai ,且U={{A1 ∪A2 ∪…∪Ak} ∪A0} ,且Ai ∩Aj =Φ。最后对k个对象集合分别进行算术平均,形成k个初始聚类中心。
近似的k平均算法已经被设计用于原始数据子集的计算。 从算法的表现上来说，它并不保证一定得到全局最优解，终于解的质量非常大程度上取决于初始化的分组。因为该算法的速度非常快。因此经常使用的一种方法是多次执行k平均算法，选择最优解。 
k平均算法的一个缺点是，分组的数目k是一个输入參数，不合适的k可能返回较差的结果。

另外，算法还如果均方误差是计算群组分散度的最佳參数。

## **三、 SVM** 

支持向量机，英文为Support  Vector  Machine，简称SV机（论文中一般简称SVM）。它是一
种监督式学习的方法，它广泛的应用于统计分类以及回归分析中。 
支持向量机属于一般化线性分类器.他们也可以觉得是提克洛夫规范化（Tikhonov Regularization）方法的一个特例.这族分类器的特点是他们可以同一时候最小化经验误差与最大化
几何边缘区.因此支持向量机也被称为最大边缘区分类器。在统计计算中，最大期望（EM）算法是在概率（probabilistic）模型中寻找參数最大似然预计的算法。当中概率模型依赖于无
法观測的隐藏变量（Latent  Variabl）。

最大期望经经常使用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。

最大期望算法经过两个步骤交替进行计算：

第一步是计算期望（E），也就是将隐藏变量象可以观測到的一样包括在内从而计算最大似然的期望值；

另外一步是最大化（M），也就是最大化在  E 步上找到的最大似然的期望值从而计算參数的最大似然预计。

M 步上找到的參数然后用于另外一个  E 步计算，这个过程不断交替进行。 
Vapnik等人在多年研究统计学习理论基础上对线性分类器提出了还有一种设计最佳准则。其原理也从线性可分说起，然后扩展到线性不可分的情况。

甚至扩展到使用非线性函数中去，这
种分类器被称为支持向量机(Support Vector Machine,简称SVM)。支持向量机的提出有非常深的理论背景。支持向量机方法是在近年来提出的一种新方法。 
SVM 的主要思想能够概括为两点： 

 (1) 它是针对线性可分情况进行分析，对于线性不可分的情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转化为高维特征空间使
其线性可分，从而使得高维特征空间採用线性算法对样本的非线性特征进行线性分析成为可能；

(2) 它基于结构风险最小化理论之上在特征空间中建构最优切割超平面，使得学习器得到全局最优化,而且在整个样本空间的期望风险以某个概率满足一定上界。 
在学习这样的方法时，首先要弄清楚这样的方法考虑问题的特点，这就要从线性可分的最简单情况讨论起，在没有弄懂其原理之前，不要急于学习线性不可分等较复杂的情况，支持向量机

在设计时。须要用到条件极值问题的求解。因此需用拉格朗日乘子理论。但对多数人来说。曾经学到的或经常使用的是约束条件为等式表示的方式。但在此要用到以不等式作为必须满足的条件，此时仅仅要了解拉格朗日理论的有关结论即可。 
介绍 
支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。

假定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是C.J.C Burges的《模式识别支持向量机指南》。van der Walt 和  Barnard 将支持向量机和其它分类器进行了比較。 
动机

有非常多个分类器(超平面）可以把数据分开，可是仅仅有一个可以达到最大切割。 我们通常希望分类的过程是一个机器学习的过程。

这些数据点并不须要是中的点。而可以是随意(统计学符号)中或者  (计算机科学符号) 的点。我们希望可以把这些点通过一个n-1维的超平面分开，通常这个被称为线性分类器。有非常多分类器都符合这个要求，可是我们还希望找到分类最佳的平面。即使得属于两个不同类的数据点间隔最大的那个面。该面亦称为最大间隔超平面。

假设我们可以找到这个面。那么这个分类器就称为最大间隔分类器。

## **四、Apriori** 

Apriori算法是种最有影响的挖掘布尔关联规则频繁项集的算法。它的核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。

在这里，全部支持度大于最小支持度的项集称为频繁项集(简称频集)，也常称为最大项目集。
在Apriori算法中，寻找最大项目集(频繁项集)的基本思想是：算法须要对数据集进行多步处理。第一步，简单统计全部含一个元素项目集出现的频数，并找出那些不小于最小支持度的项目集，即一维最大项目集。从第二步開始循环处理直到再没有最大项目集生成。循环过程是：第k步中，依据第k-1步生成的(k-1)维最大项目集产生k维侯选项目集。然后对数据库进行搜索，得到侯选项目集的项集支持度。与最小支持度进行比較，从而找到k维最大项目集。

从算法的执行过程。我们能够看出该Apriori算法的长处：简单、易理解、数据要求低。然而我们也能够看到Apriori算法的缺点：

(1)在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该參与组合的元素;

(2)每次计算项集的支持度时，都对数据库D中的所有记录进行了一遍扫描比較。假设是一个大型的数据库的话，这样的扫描比較会大大添加计算机系统的I/O开销。而这样的代价是随着数据库的记录的添加呈现出几何级数的添加。

因此人们開始寻求更好性能的算法。如F-P算法。

## **五、最大期望算法EM** 

 （Expectation-maximization algorithm。又译期望最大化算法）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，參数的最大似然预计。
在统计计算中，最大期望（EM）算法是在概率模型中寻找參数最大似然预计或者最大后验预计的算法。当中概率模型依赖于无法观測的隐藏变量（Latent Variable）。最大期望经经常使用在机器学习和计算机视觉的数据聚类（Data Clustering）领域。

最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有预计值，计算其最大似然预计值；第二步是最大化（M）。最大化在 E 步上求得的最大似然值来计算參数的值。M 步上找到的參数预计值被用于下一个 E 步计算中，这个过程不断交替进行。

 最大期望过程说明 我们用  表示可以观察到的不完整的变量值，用  表示无法观察到的变量值，这样  和  一起组成了完整的数据。可能是实际測量丢失的数据，也可能是可以简化问题的隐藏变量，假设它的值可以知道的话。比如，在混合模型（Mixture Model）中，假设“产生”样本的混合元素成分已知的话最大似然公式将变得更加便利（參见以下的样例）。  

预计无法观測的数据 让  代表矢量 :  定义的參数的所有数据的概率分布（连续情况下）或者概率聚类函数（离散情况下），那么从这个函数就能够得到所有数据的最大似然值，另外。在给定的观察到的数据条件下未知数据的条件分布能够表示为：

![](C:\Users\admin\Desktop\1.jpg)

## **六、PageRank** 

PageRank。网页排名，又称网页级别、Google左側排名或佩奇排名，是一种由搜索引擎依据网页之间相互的超链接计算的技术，而作为网页排名的要素之中的一个，以Google公司创办人拉里·佩奇（Larry Page）之姓来命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是常常被用来评估网页优化的成效因素之中的一个。Google的创始人拉里·佩奇和谢尔盖·布林于1998年在斯坦福大学发明了这项技术。

PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。

Google把从A页面到B页面的链接解释为A页面给B页面投票。Google依据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。

简单的说，一个高等级的页面能够使其它低等级页面的等级提升。

PageRank让链接来"投票"
一个页面的“得票数”由全部链向它的页面的重要性来决定，到一个页面的超链接相当于对该页投一票。一个页面的PageRank是由全部链向它的页面（“链入页面”）的重要性经过递归算法得到的。一个有较多链入的页面会有较高的等级，相反假设一个页面没有不论什么链入页面。那么它没有等级。
2005年初。Google为网页链接推出一项新属性nofollow，使得站点管理员和网志作者能够做出一些Google不计票的链接，也就是说这些链接不算作"投票"。nofollow的设置能够抵制垃圾评论。
Google工具条上的PageRank指标从0到10。它似乎是一个对数标度算法。

### 1.PageRank 

基本思想：假设网页T存在一个指向网页A的连接，则表明T的全部者觉得A比較重要。从而把T的一部分重要性得分赋予A。这个重要性得分值为：PR（T）/C(T) 
当中PR（T）为T的PageRank值。C(T)为T的出链数。则A的PageRank值为一系列类似于T的页面重要性得分值的累加。 
长处：是一个与查询无关的静态算法，全部网页的PageRank值通过离线计算获得；有效降低在线查询时的计算量，极大降低了查询响应时间。

不足：人们的查询具有主题特征，PageRank忽略了主题相关性，导致结果的相关性和主题性减少；另外。PageRank有非常严重的对新网页的鄙视。 

### 2.Topic-Sensitive PageRank（主题敏感的PageRank） 

基本思想：针对PageRank对主题的忽略而提出。核心思想：通过离线计算出一个  PageRank向量集合。该集合中的每个向量与某一主题相关，即计算某个页面关于不同主题的得分。
主要分为两个阶段：主题相关的PageRank向量集合的计算和在线查询时主题的确定。 

长处：依据用户的查询请求和相关上下文推断用户查询相关的主题（用户的兴趣）返回查询结果准确性高。 
不足：没有利用主题的相关性来提高链接得分的准确性。

### 3.Hilltop 

基本思想：与PageRank的不同之处：仅考虑专家页面的链接。主要包含两个步骤：专家页面搜索和目标页面排序。 
长处：相关性强，结果准确。

不足：专家页面的搜索和确定对算法起关键作用，专家页面的质量决定了算法的准确性，而
专家页面的质量和公平性难以保证。忽略了大量非专家页面的影响，不能反应整个Internet的民意；当没有足够的专家页面存在时，返回空，所以Hilltop适合对于查询排序进行求精。 
那么影响google PageRank的因素有哪些呢? 
1 与pr高的站点做链接: 
2 内容质量高的站点链接 
3增加搜索引擎分类文件夹 
4 增加免费开源文件夹 
5 你的链接出如今流量大、知名度高、频繁更新的重要站点上 
6 google对DPF格式的文件比較看重。 
7 安装Google工具条 
8 域名和tilte标题出现关键词与meta标签等 
9 反向连接数量和反向连接的等级 
10 Google抓取您站点的页面数量 
11导出链接数量

##  **七、AdaBoost** 

AdaBoost。是英文"Adaptive Boosting"（自适应增强）的缩写

AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。AdaBoost方法对于噪声数据和异常数据非常敏感。但在一些问题中。AdaBoost方法相对于大多数其他学习算法而言。不会非常easy出现过拟合现象。

AdaBoost方法中使用的分类器可能非常弱（比方出现非常大错误率），但仅仅要它的分类效果比随机好一点（比方两类问题分类错误率略小于0.5），就行改善终于得到的模型。而错误率高于随机分类器的弱分类器也是实用的，由于在终于得到的多个分类器的线性组合中，可以给它们赋予负系数，相同也能提升分类效果。
AdaBoost方法是一种迭代算法。在每一轮中增加一个新的弱分类器，直到达到某个预定的足够小的错误率。每个训练样本都被赋予一个权重。表明它被某个分类器选入训练集的概率。

假设某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被减少；

相反。假设某个样本点没有被准确地分类，那么它的权重就得到提高。通过这种方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。

在详细实现上，最初令每一个样本的权重都相等，对于第k次迭代操作。我们就依据这些权重来选取样本点，进而训练分类器Ck。然后就依据这个分类器，来提高被它分错的的样本的权重，并减少被正确分类的样本权重。

然后，权重更新过的样本集被用于训练下一个分类器Ck[2]。整个训练过程如此迭代地进行下去。

Adaboost算法的详细过程例如以下： 
\1. 给定训练样本集  ，当中  分别相应于正例样本和负例样本。  为训练的最大循环次数； 
\2. 初始化样本权重  ，即为训练样本的初始概率分布。 
\3. 第一次迭代： 
(1)  训练样本的概率分布  下，训练弱分类器： 
(2) 计算弱分类器的错误率： 
(3) 选取  。使得  最小 
(4) 更新样本权重： 
(5) 终于得到的强分类器： 
Adaboost算法是经过调整的Boosting算法，其可以对弱学习得到的弱分类器的错误进行适应
性调整。上述算法中迭代了次的主循环，每一次循环依据当前的权重分布对样本x定一个分
布P，然后对这个分布下的样本使用若学习算法得到一个错误率为的弱分类器  。对于这个算
法定义的弱学习算法，对全部的  ，都有，而这个错误率的上限并不须要事先知道，实际上。

每一次迭代。都要对权重进行更新。

更新的规则是：减小弱分类器分类效果较好的数据的概
率。增大弱分类器分类效果较差的数据的概率。

终于的分类器是个弱分类器的加权平均。

## **八、kNN** 

1、K近邻(k-Nearest  Neighbor。KNN)分类算法。是一个理论上比較成熟的方法。也是最简单的机器学习算法之中的一个。该方法的思路是：假设一个样本在特征空间中的k个最相似(即特征空
间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
2、KNN算法中，所选择的邻居都是已经正确分类的对象。

该方法在定类决策上仅仅根据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

 KNN方法尽管从原理上也依赖于极限定理。但在类别决策时，仅仅与极少量的相邻样本有关。因为KNN方法主要靠周围有限的邻近的样本。
而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其它方法更为适合。

3、KNN算法不仅能够用于分类，还能够用于回归。通过找出一个样本的k个近期邻居，将这些邻居的属性的平均值赋给该样本，就能够得到该样本的属性。

更实用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比。 
4、该算法在分类时有个基本的不足是，当样本不平衡时，如一个类的样本容量非常大，而其它类样本容量非常小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。因此能够採用权值的方法（和该样本距离小的邻居权值大）来改进。

​      该方法的还有一个不足之处是**计算量较大**，由于对每个待分类的文本都要计算它到全体已知样本的距离。才干求得它的K个近期邻点。

​	眼下经常使用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比較适用于样本容量比較大的类域的自己主动分类，而那些样本容量较小的类域採用这样的算法比較easy产生误分。

### 算法分类步骤例如以下：

1 首先我们事先定下k值（就是指k近邻方法的k的大小。代表对于一个待分类的数据点，我们要寻找几个它的邻居）。这边为了说明问题，我们取两个k值。分别为3和9；
2 依据事先确定的距离度量公式（如：欧氏距离）。得出待分类数据点和全部已知类别的样本点中。距离近期的k个样本。
3 统计这k个样本点中。各个类别的数量。依据k个样本中，数量最多的样本是什么类别，我们就把这个数据点定为什么类别。

训练样本是多维特征空间向量。当中每一个训练样本带有一个类别标签。

算法的训练阶段仅仅包括存储的特征向量和训练样本的标签。

在分类阶段。k是一个用户定义的常数。一个没有类别标签的向量 （查询或測试点）将被归类为最接近该点的K个样本点中最频繁使用的一类。

 普通情况下，将欧氏距离作为距离度量。可是这是仅仅适用于连续变量。

在文本分类这样的非连续变量情况下，

还有一个度量——重叠度量（或海明距离）能够用来作为度量。

通常情况下。假设运用一些特殊的算法来计算度量的话。K近邻分类精度可显著提高。如运用大边缘近期邻法或者近邻成分分析法。
“多数表决”分类的一个缺点是出现频率较多的样本将会主导測试点的预測结果。那是由于他们比較大可能出如今測试点的K邻域而測试点的属性又是通过K领域内的样本计算出来的。

解决这个缺点的方法之中的一个是在进行分类时将样本到測试点的距离考虑进去。
K值得选择
怎样选择一个最佳的K值取决于数据。普通情况下。在分类时较大的K值可以减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值能通过各种启示式技术来获取，比方，交叉验证。
噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。对于选择特征向量进行分类已经作了非常多研究。

一个普遍的做法是利用进化算法优化功能扩展[3]。另一种较普遍的方法是利用训练样本的互信息进行选择特征。

K近邻算法也适用于连续变量预计。比方适用反距离加权平均多个K近邻点确定測试点的值。该算法的功能有：
1、从目标区域抽样计算欧式或马氏距离；
2、在交叉验证后的RMSE基础上选择启示式最优的K邻域；
3、计算多元k-近期邻居的距离倒数加权平均。

## **九、Naive Baye**

### 简单介绍

贝叶斯分类的基础是概率推理。就是在各种条件的存在不确定。仅知其出现概率的情况下，怎样完毕推理和决策任务。概率推理是与确定性推理相相应的。

而朴素贝叶斯分类器是基于独立如果的，即如果样本每一个特征与其它特征都不相关。举个样例，如果一种水果其具有红。圆，直径大概4英寸等特征。该水果能够被判定为是苹果。
虽然这些特征相互依赖或者有些特征由其它特征决定。然而朴素贝叶斯分类器觉得这些属性在判定该水果是否为苹果的概率分布上独立的。朴素贝叶斯分类器依靠精确的自然概率模型，在有监督学习的样本集中能获取得很好的分类效果。在很多实际应用中。朴素贝叶斯模型參数预计使用最大似然预计方法。换而言之朴素贝叶斯模型能工作并没实用到贝叶斯概率或者不论什么贝叶斯模型。
虽然是带着这些朴素思想和过于简单化的如果，但朴素贝叶斯分类器在非常多复杂的现实情形中仍可以取得相当好的效果。2004年。一篇分析贝叶斯分类器问题的文章揭示了朴素贝叶斯分类器取得看上去不可思议的分类效果的若干理论上的原因。

虽然如此，2006年有一篇文章具体比較了各种分类方法，发现更新的方法（如boosted trees和随机森林）的性能超过了贝叶斯分类器。

朴素贝叶斯分类器的一个优势在于仅仅须要依据少量的训练数据预计出必要的參数（变量的均值和方差）。因为变量独立如果，仅仅须要预计各个变量的方法。而不须要确定整个协方差矩阵。

### 两种分类模型：

分类是将一个未知样本分到几个预先已知类的过程。

数据分类问题的解决是一个两步过程：

第一步,建立一个模型，描写叙述预先的数据集或概念集。通过分析由属性描写叙述的样本（或实例，对象等）来构造模型。

假定每个样本都有一个预先定义的类，由一个被称为类标签的属性
确定。为建立模型而被分析的数据元组形成训练数据集。该步也称作有指导的学习。 在众多的分类模型中，应用最为广泛的两种分类模型是：

### 决策树模型(Decision Tree Model)和朴素贝叶斯模型（Naive  Bayesian  Model，NBC）

决策树模型通过构造树来解决分类问题。

1、首先利用训练数据集来构造一棵决策树，一旦树建立起来，它就可为未知样本产生一个分类。在分类问题中使用决策树模型有非常多的长处。决策树便于使用。并且高效。依据决策树能够
非常easy地构造出规则，而规则通常易于解释和理解；决策树可非常好地扩展到大型数据库中，同一时候它的大小独立于数据库的大小；决策树模型的另外一大长处就是能够对有很多属性的数据集构造决策树。

决策树模型也有一些缺点，比方处理缺失数据时的困难，过度拟合问题的出现。以及忽略数据集中属性之间的相关性等。

2、和决策树模型相比，朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同一时候。NBC模型所需预计的參数非常少。对缺失数据不太敏感，算法也比較简单。
理论上，NBC模型与其它分类方法相比具有最小的误差率。

可是实际上并不是总是如此。这是由于NBC模型如果属性之间相互独立，这个如果在实际应用中往往是不成立的。这给NBC
模型的正确分类带来了一定影响。在属性个数比較多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。 

### 贝叶斯分类器特点

1、 须要知道先验概率
先验概率是计算后验概率的基础。在传统的概率理论中。先验概率能够由大量的反复实验所获得的各类样本出现的频率来近似获得。其基础是“大数定律”。这一思想称为“频率主义”。而在称为“贝叶斯主义”的数理统计学派中，他们觉得时间是单向的，很多事件的发生不具有可反复性，因此先验概率仅仅能依据对置信度的主观判定来给出，也能够说由“信仰”来确定。

2、依照获得的信息对先验概率进行修正
在没有获得不论什么信息的时候，假设要进行分类判别，仅仅能根据各类存在的先验概率。将样本划分到先验概率大的一类中。而在获得了很多其它关于样本特征的信息后。能够按照贝叶斯公式对先验概率进行修正，得到后验概率。提高分类决策的准确性和置信度。
3、分类决策存在错误率
因为贝叶斯分类是在样本取得某特征值时对它属于各类的概率进行猜測，并无法获得样本真实的类别归属情况，所以分类决策一定存在错误率，即使错误率非常低。分类错误的情况也可能发生。

## **十、分类回归树CART** 

分类回归树(CART,Classification And Regression Tree)也属于一种决策树。分类回归树是一棵二叉树，且每一个非叶子节点都有两个孩子，所以对于第一棵子树其叶子节点数比非叶子节点数多1。

### 决策树生长的核心是确定决策树的分枝准则。

#### 1、 怎样从众多的属性变量中选择一个当前的最佳分支变量。

也就是选择能使异质性下降最快的变量。
异质性的度量：GINI、TWOING、least squared deviation。
前两种主要针对分类型变量，LSD针对连续性变量。
代理划分、加权划分、先验概率

#### 2、 怎样从分支变量的众多取值中找到一个当前的最佳切割点（切割阈值）

(1) 切割阈值：
 A、数值型变量——对记录的值从小到大排序，计算每一个值作为临界点产生的子节点的异质性统计量。

可以使异质性减小程度最大的临界值便是最佳的划分点。
 B、分类型变量——列出划分为两个子集的全部可能组合。计算每种组合下生成子节点的异质性。相同。找到使异质性减小程度最大的组合作为最佳划分点。

在决策树的每个节点上我们能够按任一个属性的任一个值进行划分。按哪种划分最好呢？有3个标准能够用来衡量划分的好坏：GINI指数、双化指数、有序双化指数。

### 终止条件：

一个节点产生左右孩子后，递归地对左右孩子进行划分就可以产生分类回归树。这里的终止条件是什么？什么时候节点就能够停止分裂了？

满足下面一个即停止生长。
（1） 节点达到全然纯性；
（2） 数树的深度达到用户指定的深度。
（3） 节点中样本的个数少于用户指定的个数；
（4） 异质性指标下降的最大幅度小于用户指定的幅度。

### 剪枝

当分类回归树划分得太细时，会对噪声数据产生过拟合作用。因此我们要通过剪枝来解决。

剪枝又分为前剪枝和后剪枝：前剪枝是指在构造树的过程中就知道哪些节点能够剪掉。于是干脆不正确这些节点进行分裂。在N皇后问题和背包问题中用的都是前剪枝。上面的χ2方法也能够觉得是一种前剪枝；后剪枝是指构造出完整的决策树之后再来考查哪些子树能够剪掉。
在分类回归树中能够使用的后剪枝方法有多种，比方：代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等等。这里我们仅仅介绍代价复杂性剪枝法。

### 预測

回归树——预測值为叶节点目标变量的加权均值
分类树——某叶节点预測的分类值应是造成错判损失最小的分类值。