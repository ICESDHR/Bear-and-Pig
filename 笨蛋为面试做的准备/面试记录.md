# 阿里

###一面

- 逻辑回归 交叉熵损失函数为什么用极大似然往出算：https://blog.csdn.net/aliceyangxi1987/article/details/80532586

  **LR的全局最优解**：逻辑回归的对数似然函数是关于θ的凸函数，且有最大值，代价函数是凸函数，那么就存在全局最优解

  ```
  1.对于逻辑回归模型，假定的概率分布是伯努利分布，已知模型和一定样本的情况下，估计模型的参数，在统计学中常用的是极大似然估计方法
  
  2.由逻辑回归是一种分类算法决定。逻辑回归的假设函数是属于指数分布族，且逻辑回归的样本给予满足伯努利分布而进行训练的。最大似然估计的出发点就是使得当前样本发生的可能性最大化，反向支持逻辑回归样本满足伯努利分布。
  ```

  1. 因为我们想要让 每一个 样本的预测都要得到最大的概率， 
     即将所有的样本预测后的概率进行相乘都最大，也就是极大似然函数.
  2. 对极大似然函数取对数以后相当于对数损失函数， 
     由上面 梯度更新 的公式可以看出， 
     对数损失函数的训练求解参数的速度是比较快的， 
     而且更新速度只和x，y有关，比较的稳定，
  3. 为什么不用平方损失函数 
     如果使用平方损失函数，梯度更新的速度会和 sigmod 函数的梯度相关，sigmod 函数在定义域内的梯度都不大于0.25，导致训练速度会非常慢。 
     而且平方损失会导致损失函数是 theta 的非凸函数，不利于求解，因为非凸函数存在很多局部最优解。

- python * **的区别

- 逻辑回归进行分类时如何求得最优解

- 10亿个大小平均长度为10的单词，给一个字符串，找到以其为前缀的所有单词

- 编程：

  - 把排序数组转换为高度最小的二叉搜索树https://blog.csdn.net/guoziqing506/article/details/51184466 leetcode108

    - 拓展：排序列表转换为二分查找树https://blog.csdn.net/guoziqing506/article/details/51212234

  - 一个字符串转变为另一个字符串修改的次数 leetcode72

    https://blog.csdn.net/yangjingjing9/article/details/76724291 代码

     http://www.cnblogs.com/pandora/archive/2009/12/20/levenshtein_distance.html 一些讲解



- CNN过拟合



# 海康威视

- 电话面
  - svm的约束条件

  - 核函数需要满足什么条件才能成为核函数

    李航书P118和P122:通常所说的核函数就是正定核函数

    ![IMG_0320.jpg](https://i.loli.net/2018/08/02/5b63102e6a9c5.jpg) 



# 小米

- hashmap

- FM（因式分解机？）

- java中==和equal()的区别

- 什么时候欠拟合什么时候过拟合

  https://blog.csdn.net/willduan1/article/details/53070777（最好背下来）

- 是否会跟踪一些最新的机器学习方面的研究成果



# 作业帮

- java和python的跨平台之间的异同
- 凸包问题（回顾具体做法和时间复杂度）
- 