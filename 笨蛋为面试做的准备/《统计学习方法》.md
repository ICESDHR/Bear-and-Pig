[TOC]

# 第1章 统计学习方法概论

### 范数

- https://www.zhihu.com/question/20473040?utm_campaign=rss&utm_medium=rss&utm_source=rss&utm_content=title


- **向量范数**

  **1-范数**： $||x||_1 = \sum_{i=1}^N|x_i|$，即向量元素绝对值之和，matlab调用函数norm(x, 1) 。

  **2-范数**：[$||\textbf{x}||_2 =\sqrt{\sum_{i=1}^Nx_i^2}$]，Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方，matlab调用函数norm(x, 2)。

  **![\infty](https://www.zhihu.com/equation?tex=%5Cinfty)-范数**：$||\textbf{x}||_\infty = \max_{i}|x_i|$，即所有向量元素绝对值中的最大值，matlab调用函数norm(x, inf)。

  **![-\infty](https://www.zhihu.com/equation?tex=-%5Cinfty)-范数**：$||\textbf{x}||_{-\infty}=\min_i|x_i|$，即所有向量元素绝对值中的最小值，matlab调用函数norm(x, -inf)。

  **p-范数**：$||\textbf{x}||_p = (\sum_{i=1}^N|x_i|^p)^{\frac{1}{p}}$，即向量元素绝对值的p次方和的1/p次幂，matlab调用函数norm(x, p)。

- **矩阵范数**

  **1-范数**：$||A||_1 = \max_j\sum_{i=1}^m|a_{i,j}|$， 列和范数，即所有矩阵列向量绝对值之和的最大值，matlab调用函数norm(A, 1)。

  **2-范数**：$||A||_2 = \sqrt{\lambda_1}$，![\lambda<br/>](https://www.zhihu.com/equation?tex=%5Clambda%3Cbr%2F%3E)为![A^TA](https://www.zhihu.com/equation?tex=A%5ETA)的最大特征值。

  **![\infty](https://www.zhihu.com/equation?tex=%5Cinfty)-范数**：![||A||_\infty = \max_i\sum_{j=1}^N|a_{i,j}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty+%3D+%5Cmax_i%5Csum_%7Bj%3D1%7D%5EN%7Ca_%7Bi%2Cj%7D%7C)，行和范数，即所有矩阵行向量绝对值之和的最大值，matlab调用函数norm(A, inf)。

  **F-范数**：![||A||_F=\left(\sum_{i=1}^m\sum_{j=1}^n|a_{i,j}|^2\right)^{\frac{1}{2}}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_F%3D%5Cleft%28%5Csum_%7Bi%3D1%7D%5Em%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bi%2Cj%7D%7C%5E2%5Cright%29%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D)，Frobenius范数，即矩阵元素绝对值的平方和再开平方，matlab调用函数norm(A, ’fro‘)。

  **核范数**：$||A||_* = \sum_{i=1}^{n}\lambda_i, \lambda_i$是A的奇异值，即奇异值之和。





# 第2章 感知机

### Gram矩阵

$G={ A }^{ T }A=\begin{bmatrix} { a }_{ 1 }^{ T } \\ { a }_{ 2 }^{ T } \\ · \\ · \\ { a }_{ n }^{ T } \end{bmatrix}\begin{bmatrix} { a }_{ 1 } & { a }_{ 2 } & · & · & { a }_{ n } \end{bmatrix}=\begin{bmatrix} { a }_{ 1 }^{ T }{ a }_{ 1 } & { a }_{ 1 }^{ T }{ a }_{ 2 } & · & · & { a }_{ 1 }^{ T }{ a }_{ n } \\ { a }_{ 2 }^{ T }{ a }_{ 1 } & { a }_{ 2 }^{ T }{ a }_{ 2 } & · & · & { a }_{ 2 }^{ T }{ a }_{ n } \\ · & · & · & · & · \\ { a }_{ n }^{ T }{ a }_{ 1 } & { a }_{ n }^{ T }{ a }_{ 2 } & · & · & { a }_{ n }^{ T }{ a }_{ n } \end{bmatrix}$







# 第3章 k近邻算法

### kd树

https://blog.csdn.net/app_12062011/article/details/51986805



# 第4章 朴素贝叶斯法





# 第5章 决策树

- ID3和C4.5称为决策树，CART称为**分类与回归树**

### CART的剪枝过程

https://blog.csdn.net/u014688145/article/details/53326910这个算是解释比较好的帖子了，笨蛋只看了剪枝那部分，前面的部分感觉李航的书上已经讲得比较清楚了。



#第6章 逻辑斯蒂回归(LR回归)与最大熵模型

- LR回归，虽然这个算法从名字上来看，是回归算法，但其实际上是一个分类算法
- 逻辑回归与线性回归的关系https://blog.csdn.net/han_xiaoyang/article/details/49123419
- 形状参数的解释（对应于博客中的s）https://blog.csdn.net/daunxx/article/details/51816588笨蛋只看了第一部分



# 第7章 支持向量机







# 第8章 提升方法







# 第9章 EM算法及其推广







# 第10章 隐马尔可夫模型







# 第11章 条件随机场













##PS

- 介绍一下“似然性”的概念。目标函数f(x)=P(+1|x)f(x)=P(+1|x)，如果我们找到了hypothesis很接近target function。也就是说，在所有的Hypothesis集合中找到一个hypothesis与target function最接近，能产生同样的数据集D，包含y输出label，则称这个hypothesis是最大似然likelihood。 —— 台湾大学林轩田机器学习基石课程学习笔记10—Logistic Regression