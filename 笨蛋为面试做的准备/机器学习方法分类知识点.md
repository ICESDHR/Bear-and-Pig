[TOC]

# 机器学习知识点

## 逻辑回归(LR)

https://www.cnblogs.com/ModifyRong/p/7739955.html    背会！！！！！极品啊啊啊啊

### 逻辑回归为什么使用Sigmod作为激活函数？

https://blog.csdn.net/cloud_xiaobai/article/details/72152480    这个比较好

https://blog.csdn.net/qq_26598445/article/details/81291954

总结：因为逻辑回归服从伯努利分布，所以它是指数族分布，又因为逻辑回归是广义线性模型，需要满足广义线性模型的三个条件，通过假设（2）推导出其激活函数是sigmod函数。



## LR和SVM的联系与区别：

- 联系：  
  - LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）  
  - 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 
- 区别：  
  - LR是参数模型，SVM是非参数模型。  
  - 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。  
  - SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。  
  - 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 
  - logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

## 决策树总结

### ID3



### C4.5



### CART



### 树的融合



## L1 L2

- 作用

  - L1：（L1正则化——Lasso回归）
    - 特征选择
    - 可解释性
  - L2：（L2正则化——Ridge回归）
    - 防止过拟合，提高模型的泛化能力
    - （可以不说）从优化或者数值计算的角度来说L2有助于处理condition number不好的情况下矩阵求逆困难的问题

- 为什么有这样的作用

  将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解：

  ![20140504123020546.png](https://i.loli.net/2018/08/20/5b7a5d4f7b380.png)

- 求L1的优化方法

  https://www.cnblogs.com/pinard/p/6018889.html

  - 坐标下降法

    

  ​	

  - 最小角回归法	



## 集成学习

集成学习分为两类：

- 个体学习器间存在强依赖关系、必须串行生成序列化方法。典型代表是Boosting
- 个体学习器间不存在强依赖关系、可同时生成的并行化方法。典型代表是Bagging和随机森林



### Boosting（提升方法）

- Boosting是一族可将弱学习器提升为强学习器的算法
- 对提升方法来说有两个问题需要回答：
  1. 在每一轮如何改变训练数据的权值或概率分布
  2. 如何将弱分类器组合成一个强分类器
- Boosting算法的“重赋权法”和“重采样法”看西瓜书P177
- ==重启动==看西瓜书P177
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。



#### AdaBoost算法（李航书P138，P140例子很好懂推荐看）

- AdaBoost算法的特点：
  1. 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用
  2. 利用基本分类器的线性组合构建最终的分类器
- 标准AdaBoost只适用于二分类任务，Bagging能不经修改地用于多分类、回归等任务



#### 提升树模型

以决策树为基函数的提升方法称为提升树（boosting tree）

##### 提升树算法

- 对于二分类问题，提升树算法只需将AdaBoost算法中的基分类器限制为二分类树即可
- 回归问题的提升树算法（对于平方损失函数来说就是不停地拟合残差）看李航书P148以及例8.2
- 当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，需要使用==梯度提升算法==相当于不停地拟合残差的近似值

##### GBDT（梯度提升树）

- 推荐博客 
  - https://www.cnblogs.com/ModifyRong/p/7744987.html   [机器学习算法GBDT的面试要点总结-上篇](https://www.cnblogs.com/ModifyRong/p/7744987.html)
  - https://blog.csdn.net/w28971023/article/details/8240756
  - http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/ 基本把陈天奇论文的主体翻译了一下，先看这个在读论文会好理解一些

##### XGBoost

- http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/   xgboost相比传统gbdt有何不同？xgboost为什么快？xgboost如何支持并行？
- 推荐博客 https://www.xianjichina.com/news/details_67971.html

##### GBDT与XGBoost的不同









##### LightGBM

- https://blog.csdn.net/qq_33638791/article/details/78972853
- http://lightgbm.apachecn.org/cn/latest/Installation-Guide.html#osx



### Bagging

- Bagging是并行式集成学习方法最著名的代表，它直接基于西瓜书P27介绍的==自助采样法（bootstrap sampling）==
- 自助采样过程给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2％的样本，剩下约36.8％的样本可用作验证集来对泛化性能进行“包外估计”
- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树。神经网络等易受样本扰动的学习器上效用更为明显



#### 随机森林

- 随机森林是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后在从这个子集中选择一个最有属性用于划分。
- 随机森林的训练效率常==优于==Bagging



### 模型融合（stacking）

https://zhuanlan.zhihu.com/p/25836678

https://blog.csdn.net/wstcjf/article/details/77989963 详解stacking过程

https://blog.csdn.net/qq_18916311/article/details/78557722 这个是最好的 感觉

https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/ 如何在 Kaggle 首战中进入前 10%

https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python 泰坦尼克的例子 用了stacking

![img](https://img-blog.csdn.net/20171117094740886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTg5MTYzMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



## 归一化

- 基于树的方法不需要进行特征的归一化。例如随机森林，bagging与boosting等方法。如果是基于参数的模型或者基于距离的模型，因为需要对参数或者距离进行计算，都需要进行归一化。



## EM算法

- https://blog.csdn.net/zhihua_oba/article/details/73776553

- EM算法的应用：[k-means算法](https://blog.csdn.net/zhihua_oba/article/details/73832614)是EM算法思想的体现，E步骤为聚类过程，M步骤为更新类簇中心。

- EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation Maximization Algorithm）。EM算法受到缺失思想影响，最初是为了解决数据缺失情况下的参数估计问题，其算法基础和收敛有效性等问题在Dempster，Laird和Rubin三人于1977年所做的文章Maximum likelihood from incomplete data via the EM algorithm中给出了详细的阐述。其基本思想是：首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。    EM算法作为一种数据添加算法，在近几十年得到迅速的发展，主要源于当前科学研究以及各方面实际应用中数据量越来越大的情况下，经常存在数据缺失或者不可用的的问题，这时候直接处理数据比较困难，而数据添加办法有很多种，常用的有神经网络拟合、添补法、卡尔曼滤波法等等，但是EM算法之所以能迅速普及主要源于它算法简单，稳定上升的步骤能非常可靠地找到“最优的收敛值”。随着理论的发展，EM算法己经不单单用在处理缺失数据的问题，运用这种思想，它所能处理的问题更加广泛。有时候缺失数据并非是真的缺少了，而是为了简化问题而采取的策略，这时EM算法被称为数据添加技术，所添加的数据通常被称为“潜在数据”，复杂的问题通过引入恰当的潜在数据，能够有效地解决我们的问题。
-  **EM算法优缺点以及应用**    
  - 优点：简介中已有介绍，这里不再赘述。    
  - 缺点：对初始值敏感：EM算法需要初始化参数θθ，而参数θθ的选择直接影响收敛效率以及能否得到全局最优解。 

## 对极大似然的理解

- 我们已知的有两个：样本服从的分布模型、随机抽取的样本；我们未知的有一个：模型的参数。根据已知条件，通过极大似然估计，求出未知参数。总的来说：极大似然估计就是用来估计模型参数的统计学方法。 
- ![IMG_0329.jpg](https://i.loli.net/2018/08/05/5b664e3ceb349.jpg) 

## 反向传播算法推导

![IMG_0365.jpg](https://i.loli.net/2018/08/15/5b738ca59a96d.jpg)



##支持向量机——SMO（序列最小最优化算法）算法



# 深度学习知识点

### 卷积神经网络(CNN)

- channel表示色彩通道

- 一个卷积神经网络主要由以下5种结构组成：

  - 输入层。
  - 卷积层。卷积层试图将神经网络中的每一块进行更加深入地分析从而得到抽象程度更高的特征。一般来说卷积层处理过得节点矩阵会变得更深。
  - 池化层。池化层神经网络不会改变三维矩阵的深度，但是它可以缩小矩阵的大小。

- 深度学习防止过拟合方法

  https://blog.csdn.net/NoviceRoad/article/details/75734048

  - 关于dropout

    https://blog.csdn.net/stdcoutzyx/article/details/49022443





### 如何训练

![](https://ws2.sinaimg.cn/large/006tKfTcly1ftqr8640etj30i00dtdiu.jpg)

#### Choosing proper loss

- **when using softmax output layer,choose cross entropy**, why?

  ![FD588103-5631-464D-93F3-125A93D77FE0](https://ws1.sinaimg.cn/large/006tKfTcly1ftqr94zy4ej30il0bejvo.jpg)

#### Mini-batch

![20500BC0-39FB-435C-BBC9-BDAF4310E1F8](https://ws2.sinaimg.cn/large/006tKfTcly1ftqrg1t4ijj30gm096ab8.jpg)

#### New activation function

- 梯度消失问题（查资料补充）

  ![19997788-B3CF-4280-9F91-1242A6E0698A](https://ws3.sinaimg.cn/large/006tKfTcly1ftqsbuf72rj30ia0d3mzq.jpg)

**问**：使用ReLU作为激活函数的原因？

答：1. 计算快 2.Biological reason 3.Infinite sigmoid with different biases 4.为了解决梯度消失的问题

#### Adaptive Learning Rate





#### Momentum





### 过拟合

深度学习防止过拟合方法

https://blog.csdn.net/NoviceRoad/article/details/75734048

- 关于dropout

  https://blog.csdn.net/stdcoutzyx/article/details/49022443

#### 为什么会过拟合？

- 训练集和测试集不一致

  ![B8BFDDCA-824F-4DF4-B220-705A00A7587A](https://ws2.sinaimg.cn/large/006tKfTcly1ftru2s3yvoj30gd04wdg8.jpg)

#### 数据增强

- have more training data

- **create** more training data

  ![](https://ws1.sinaimg.cn/large/006tKfTcly1ftr1jzbqblj30lo06eq3r.jpg)



#### Early Stopping





#### Regularization

- Weight decay(权重衰减)



#### Dropout

dropout可以进一步提升模型可靠性并防止过拟合，dropou过程只在训练时使用。

dropout一般只在全连接层而不是卷积层或池化层使用

![D975E167-91B8-47A4-B158-624E9158D45C.png](https://i.loli.net/2018/08/15/5b7393a7c3239.png)

![3BF5B882-0EFE-4225-9230-D060FAF756E5.png](https://i.loli.net/2018/08/15/5b7393eac3ad7.png)

![9568C14B-DCEA-4844-A92C-93452ADD8807.png](https://i.loli.net/2018/08/15/5b7394244c5fd.png)



#### BN（Batch Normalization 批归一化）

- Note：Batch normalization can't be applied on small batch.

- https://www.bilibili.com/video/av16000304?from=search&seid=5471483546850822189  莫烦讲解

- https://www.bilibili.com/video/av16540598?from=search&seid=5471483546850822189  李宏毅讲解

- ![E65B3991-2388-428D-8B35-712A48CA91CC.png](https://i.loli.net/2018/08/16/5b74e64d364bd.png)

  ![4847EBAB-AB1B-4751-BB13-63EE9B451039.png](https://i.loli.net/2018/08/16/5b74e7734f204.png)

  ![CF9AF8E2-56BA-44B3-BA2D-BB67A5C6AFBA.png](https://i.loli.net/2018/08/16/5b74e94e02b64.png)

  ![C7B290D9-A935-4E0E-BBDC-FBA07D3B5ABE.png](https://i.loli.net/2018/08/16/5b74ea3ab1e9c.png)

  

##### ReLU函数进行Batch Normalization意义分析

我们都知道，正则化是一种防止训练参数过拟合的一种极为有效的方式。激活函数的主要作用是提升圣经网络的非线性性。之前常用的激活函数，如Sigmoid, tanh函数，由于其饱和区的接近于0，因此需要将其进行正则化，转换为正态分布，将数据大部分规范到线性范围之内，然后通过两个额外的线性转换参数来进行微调，从而避免后续梯度消失问题的产生。

那么就有一个问题了，ReLU这一函数并没有上界，也就是其导数不存在趋近于0的情况，那么为什么在进行图像处理相关的神经网络的时候还是需要将其进行正则化呢？

这是因为，我们前面的讨论过程中，忽略了一个很重要的问题，那就是输入变量的分布。理论上而言，如果输入的数据是不同分布的，那么相当于我们的神经网络还要针对这种不同分布去提升网络参数的适应性，从而使得网络变得难以训练。实践表明，将输入数据规范化为正态分布确实有利于提升网络的整体效果。因此在ReLU函数之后接上了一个Batch Normalization层。



#### Dropout和BN为什么不能同时使用？

http://www.sohu.com/a/218382470_465975

Dropout 与 BN 之间冲突的关键是网络状态切换过程中存在神经方差的（neural variance）不一致行为。试想若有图一中的神经响应 X，当网络从训练转为测试时，Dropout 可以通过其随机失活保留率（即 p）来缩放响应，并在学习中改变神经元的方差，而 BN 仍然维持 X 的统计滑动方差。这种方差不匹配可能导致数值不稳定（见图 1 中的红色曲线）。而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能。简单起见，作者们将这一现象命名为「方差偏移」。事实上，如果没有 Dropout，那么实际前馈中的神经元方差将与 BN 所累计的滑动方差非常接近（见图 1 中的蓝色曲线），这也保证了其较高的测试准确率。





#### Network Structure





### 梯度消失(弥散)和梯度爆炸

https://blog.csdn.net/u013250416/article/details/81410693  梯度消失与梯度爆炸、Loss为Nan的原因

- 梯度消失解决方法

  1、**激活函数**：使用ReLU,maxout激活函数等替代sigmoid。  区别：（1）sigmoid函数值在[0,1],ReLU函数值在[0,+无穷]，所以sigmoid函数可以描述概率，ReLU适合用来描述实数；（2）sigmoid函数的梯度随着x的增大或减小消失，而ReLU不会。  ReLU的导数为1，所以f’(zj) = 1。标准的sigmoid输出不具备稀疏性，需要通过惩罚因子来训练一堆接近于0的冗余数据，从而产生稀疏数据，比如L1，L2或者student-t作为惩罚因子，进行regularization。而ReLU为线性修正，是purelin的折线版，作用是如果计算输出小于0，就让它等于0，否则保持原来的值，这是一种简单粗暴地强制某些数据为0的方法，然而经实践证明，训练后的网络完全具备适度的稀疏性，而且训练后的可视化效果和传统pre-training的效果很相似。这说明了ReLU具备引导适度稀疏的能力。  

  2、**BN**层  BN（Batch Normalization）层的作用  （1）加速收敛（2）控制过拟合，可以少用或不用Dropout和正则（3）降低网络对初始化权重不敏感（4）允许使用较大的学习率  在每一层输入的时候，加个BN预处理操作。BN应作用在非线性映射前，即对x=Wu+b做规范化。在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法 

  3、**有效的权值初始化方法**

- 梯度爆炸解决方法

  https://blog.csdn.net/tz_zs/article/details/78925999

  

### CNN

#### 入门讲解

https://blog.csdn.net/hjimce/article/details/47323463

https://blog.csdn.net/hjimce/article/details/51761865

直接看这两篇就行了！很好



#### 为什么选用CNN





#### 优化

dropout一般只在全连接层而不是卷积层或池化层使用



#### 调参





#### 了解哪些池化操作？





### 优化方法

详见专门的.md——**梯度下降优化算法综述**

https://blog.csdn.net/google19890102/article/details/69942970

SGD、Momentum和Nesterov加速梯度下降法（Nesterov accelerated gradient，NAG）都是整体梯度更新方法

之后的方法都是为了适应每一个单独参数更新而设计的方法

| ![img](http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif) | ![img](https://ws3.sinaimg.cn/large/006tKfTcly1ftqu03oexvg30h80dc1ca.gif) |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|                 (a)损失去面的等高线上SGD优化                 |                     (b)在鞍点处的SGD优化                     |

#### SGD



#### Momentum(把喝醉的人放在下坡路上)

![7AE4C312-4DD7-479C-8029-42C166BAD915.png](https://i.loli.net/2018/08/17/5b762b33c2ddc.png)

#### AdaGrad（不舒服的鞋子编程阻力）

![0D8EF2F9-3D7A-44BF-90EF-1DA6D0731E54.png](https://i.loli.net/2018/08/17/5b762b9c7aa05.png)

#### RMSProp

![ED7EB324-B533-48D8-BAC3-C380692F5702.png](https://i.loli.net/2018/08/17/5b762c45d8b2e.png)

#### Adam（自适应矩估计）

![C9689A9B-9D2D-4697-83E5-FF28A5C4B6E4.png](https://i.loli.net/2018/08/17/5b762c96eca81.png)

