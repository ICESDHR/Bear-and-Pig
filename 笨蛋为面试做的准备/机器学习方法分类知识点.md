[TOC]

# 集成学习

集成学习分为两类：

- 个体学习器间存在强依赖关系、必须串行生成序列化方法。典型代表是Boosting
- 个体学习器间不存在强依赖关系、可同时生成的并行化方法。典型代表是Bagging和随机森林

## Boosting（提升方法）

- Boosting是一族可将弱学习器提升为强学习器的算法
- 对提升方法来说有两个问题需要回答：
  1. 在每一轮如何改变训练数据的权值或概率分布
  2. 如何将弱分类器组合成一个强分类器
- Boosting算法的“重赋权法”和“重采样法”看西瓜书P177
- ==重启动==看西瓜书P177
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。



### AdaBoost算法（李航书P138，P140例子很好懂推荐看）

- AdaBoost算法的特点：
  1. 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用
  2. 利用基本分类器的线性组合构建最终的分类器
- 标准AdaBoost只适用于二分类任务，Bagging能不经修改地用于多分类、回归等任务



### 提升树模型

以决策树为基函数的提升方法称为提升树（boosting tree）

#### 提升树算法

- 对于二分类问题，提升树算法只需将AdaBoost算法中的基分类器限制为二分类树即可
- 回归问题的提升树算法（对于平方损失函数来说就是不停地拟合残差）看李航书P148以及例8.2
- 当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，需要使用==梯度提升算法==相当于不停地拟合残差的近似值

#### GBDT（梯度提升树）

- 推荐博客 
  - https://blog.csdn.net/w28971023/article/details/8240756
  - http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/ 基本把陈天奇论文的主体翻译了一下，先看这个在读论文会好理解一些

#### XGBoost

- 推荐博客 https://www.xianjichina.com/news/details_67971.html

#### LightGBM

- https://blog.csdn.net/qq_33638791/article/details/78972853
- http://lightgbm.apachecn.org/cn/latest/Installation-Guide.html#osx



## Bagging

- Bagging是并行式集成学习方法最著名的代表，它直接基于西瓜书P27介绍的==自助采样法（bootstrap sampling）==
- 自助采样过程给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2％的样本，剩下约36.8％的样本可用作验证集来对泛化性能进行“包外估计”
- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树。神经网络等易受样本扰动的学习器上效用更为明显



### 随机森林

- 随机森林是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后在从这个子集中选择一个最有属性用于划分。
- 随机森林的训练效率常==优于==Bagging





## 模型融合（stacking）

https://zhuanlan.zhihu.com/p/25836678

https://blog.csdn.net/wstcjf/article/details/77989963 详解stacking过程

https://blog.csdn.net/qq_18916311/article/details/78557722 这个是最好的 感觉

https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/ 如何在 Kaggle 首战中进入前 10%

https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python 泰坦尼克的例子 用了stacking

![img](https://img-blog.csdn.net/20171117094740886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTg5MTYzMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)





# 归一化

- 基于树的方法不需要进行特征的归一化。例如随机森林，bagging与boosting等方法。如果是基于参数的模型或者基于距离的模型，因为需要对参数或者距离进行计算，都需要进行归一化。



# EM算法

- https://blog.csdn.net/zhihua_oba/article/details/73776553

- EM算法的应用：[k-means算法](https://blog.csdn.net/zhihua_oba/article/details/73832614)是EM算法思想的体现，E步骤为聚类过程，M步骤为更新类簇中心。

- EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation Maximization Algorithm）。EM算法受到缺失思想影响，最初是为了解决数据缺失情况下的参数估计问题，其算法基础和收敛有效性等问题在Dempster，Laird和Rubin三人于1977年所做的文章Maximum likelihood from incomplete data via the EM algorithm中给出了详细的阐述。其基本思想是：首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。    EM算法作为一种数据添加算法，在近几十年得到迅速的发展，主要源于当前科学研究以及各方面实际应用中数据量越来越大的情况下，经常存在数据缺失或者不可用的的问题，这时候直接处理数据比较困难，而数据添加办法有很多种，常用的有神经网络拟合、添补法、卡尔曼滤波法等等，但是EM算法之所以能迅速普及主要源于它算法简单，稳定上升的步骤能非常可靠地找到“最优的收敛值”。随着理论的发展，EM算法己经不单单用在处理缺失数据的问题，运用这种思想，它所能处理的问题更加广泛。有时候缺失数据并非是真的缺少了，而是为了简化问题而采取的策略，这时EM算法被称为数据添加技术，所添加的数据通常被称为“潜在数据”，复杂的问题通过引入恰当的潜在数据，能够有效地解决我们的问题。
-  **EM算法优缺点以及应用**    
  - 优点：简介中已有介绍，这里不再赘述。    
  - 缺点：对初始值敏感：EM算法需要初始化参数θθ，而参数θθ的选择直接影响收敛效率以及能否得到全局最优解。 

# 对极大似然的理解

- 我们已知的有两个：样本服从的分布模型、随机抽取的样本；我们未知的有一个：模型的参数。根据已知条件，通过极大似然估计，求出未知参数。总的来说：极大似然估计就是用来估计模型参数的统计学方法。 
- ![IMG_0329.jpg](https://i.loli.net/2018/08/05/5b664e3ceb349.jpg) 

