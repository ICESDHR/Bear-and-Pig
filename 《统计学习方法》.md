[TOC]

# 第1章 统计学习方法概论

### 范数

- **向量范数**

  **1-范数**： ![||x||_1 = \sum_{i=1}^N|x_i|](https://www.zhihu.com/equation?tex=%7C%7Cx%7C%7C_1+%3D+%5Csum_%7Bi%3D1%7D%5EN%7Cx_i%7C)，即向量元素绝对值之和，matlab调用函数norm(x, 1) 。

  **2-范数**：![||\textbf{x}||_2 =\sqrt{\sum_{i=1}^Nx_i^2}](https://www.zhihu.com/equation?tex=%7C%7C%5Ctextbf%7Bx%7D%7C%7C_2+%3D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5ENx_i%5E2%7D)，Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方，matlab调用函数norm(x, 2)。

  **![\infty](https://www.zhihu.com/equation?tex=%5Cinfty)-范数**：![||\textbf{x}||_\infty = \max_{i}|x_i|](https://www.zhihu.com/equation?tex=%7C%7C%5Ctextbf%7Bx%7D%7C%7C_%5Cinfty+%3D+%5Cmax_%7Bi%7D%7Cx_i%7C)，即所有向量元素绝对值中的最大值，matlab调用函数norm(x, inf)。

  **![-\infty](https://www.zhihu.com/equation?tex=-%5Cinfty)-范数**：![||\textbf{x}||_{-\infty}=\min_i|x_i|](https://www.zhihu.com/equation?tex=%7C%7C%5Ctextbf%7Bx%7D%7C%7C_%7B-%5Cinfty%7D%3D%5Cmin_i%7Cx_i%7C)，即所有向量元素绝对值中的最小值，matlab调用函数norm(x, -inf)。

  **p-范数**：![||\textbf{x}||_p = (\sum_{i=1}^N|x_i|^p)^{\frac{1}{p}}](https://www.zhihu.com/equation?tex=%7C%7C%5Ctextbf%7Bx%7D%7C%7C_p+%3D+%28%5Csum_%7Bi%3D1%7D%5EN%7Cx_i%7C%5Ep%29%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D)，即向量元素绝对值的p次方和的1/p次幂，matlab调用函数norm(x, p)。

- **矩阵范数**

  **1-范数**：![||A||_1 = \max_j\sum_{i=1}^m|a_{i,j}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_1+%3D+%5Cmax_j%5Csum_%7Bi%3D1%7D%5Em%7Ca_%7Bi%2Cj%7D%7C)， 列和范数，即所有矩阵列向量绝对值之和的最大值，matlab调用函数norm(A, 1)。

  **2-范数**：![||A||_2 = \sqrt{\lambda_1}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_2+%3D+%5Csqrt%7B%5Clambda_1%7D)，![\lambda<br/>](https://www.zhihu.com/equation?tex=%5Clambda%3Cbr%2F%3E)为![A^TA](https://www.zhihu.com/equation?tex=A%5ETA)的最大特征值。

  **![\infty](https://www.zhihu.com/equation?tex=%5Cinfty)-范数**：![||A||_\infty = \max_i\sum_{j=1}^N|a_{i,j}|](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%5Cinfty+%3D+%5Cmax_i%5Csum_%7Bj%3D1%7D%5EN%7Ca_%7Bi%2Cj%7D%7C)，行和范数，即所有矩阵行向量绝对值之和的最大值，matlab调用函数norm(A, inf)。

  **F-范数**：![||A||_F=\left(\sum_{i=1}^m\sum_{j=1}^n|a_{i,j}|^2\right)^{\frac{1}{2}}](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_F%3D%5Cleft%28%5Csum_%7Bi%3D1%7D%5Em%5Csum_%7Bj%3D1%7D%5En%7Ca_%7Bi%2Cj%7D%7C%5E2%5Cright%29%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D)，Frobenius范数，即矩阵元素绝对值的平方和再开平方，matlab调用函数norm(A, ’fro‘)。

  **核范数**：![||A||_* = \sum_{i=1}^{n}\lambda_i, \lambda_i](https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%2A+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Clambda_i%2C+%5Clambda_i)是A的奇异值，即奇异值之和。





# 第2章 感知机

### Gram矩阵

$G={ A }^{ T }A=\begin{bmatrix} { a }_{ 1 }^{ T } \\ { a }_{ 2 }^{ T } \\ · \\ · \\ { a }_{ n }^{ T } \end{bmatrix}\begin{bmatrix} { a }_{ 1 } & { a }_{ 2 } & · & · & { a }_{ n } \end{bmatrix}=\begin{bmatrix} { a }_{ 1 }^{ T }{ a }_{ 1 } & { a }_{ 1 }^{ T }{ a }_{ 2 } & · & · & { a }_{ 1 }^{ T }{ a }_{ n } \\ { a }_{ 2 }^{ T }{ a }_{ 1 } & { a }_{ 2 }^{ T }{ a }_{ 2 } & · & · & { a }_{ 2 }^{ T }{ a }_{ n } \\ · & · & · & · & · \\ { a }_{ n }^{ T }{ a }_{ 1 } & { a }_{ n }^{ T }{ a }_{ 2 } & · & · & { a }_{ n }^{ T }{ a }_{ n } \end{bmatrix}$







# 第3章 k近邻算法

### kd树

https://blog.csdn.net/app_12062011/article/details/51986805



# 第4章 朴素贝叶斯法





# 第5章 决策树

- ID3和C4.5称为决策树，CART称为**分类与回归树**

### CART的剪枝过程

https://blog.csdn.net/u014688145/article/details/53326910这个算是解释比较好的帖子了，笨蛋只看了剪枝那部分，前面的部分感觉李航的书上已经讲得比较清楚了。



#第6章 逻辑斯蒂回归(LR回归)与最大熵模型

- LR回归，虽然这个算法从名字上来看，是回归算法，但其实际上是一个分类算法
- 形状参数的解释（对应于博客中的s）https://blog.csdn.net/daunxx/article/details/51816588笨蛋只看了第一部分



# 第7章 支持向量机







# 第8章 提升方法







# 第9章 EM算法及其推广







# 第10章 隐马尔可夫模型







# 第11章 条件随机场

