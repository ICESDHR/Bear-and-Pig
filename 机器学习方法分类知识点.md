[TOC]

# 集成学习

集成学习分为两类：

- 个体学习器间存在强依赖关系、必须串行生成序列化方法。典型代表是Boosting
- 个体学习器间不存在强依赖关系、可同时生成的并行化方法。典型代表是Bagging和随机森林

## Boosting（提升方法）

- Boosting是一族可将弱学习器提升为强学习器的算法
- 对提升方法来说有两个问题需要回答：
  1. 在每一轮如何改变训练数据的权值或概率分布
  2. 如何将弱分类器组合成一个强分类器
- Boosting算法的“重赋权法”和“重采样法”看西瓜书P177
- ==重启动==看西瓜书P177
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。



### AdaBoost算法（李航书P138，P140例子很好懂推荐看）

- AdaBoost算法的特点：
  1. 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用
  2. 利用基本分类器的线性组合构建最终的分类器
- 标准AdaBoost只适用于二分类任务，Bagging能不经修改地用于多分类、回归等任务



## Bagging

- Bagging是并行式集成学习方法最著名的代表，它直接基于西瓜书P27介绍的==自助采样法（bootstrap sampling）==
- 自助采样过程给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2％的样本，剩下约36.8％的样本可用作验证集来对泛化性能进行“包外估计”
- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树。神经网络等易受样本扰动的学习器上效用更为明显



## 随机森林

- 随机森林是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后在从这个子集中选择一个最有属性用于划分。
- 随机森林的训练效率常==优于==Bagging



## 提升树模型

以决策树为基函数的提升方法称为提升树（boosting tree）



## 提升树算法

- 对于二分类问题，提升树算法只需将AdaBoost算法中的基分类器限制为二分类树即可
- 回归问题的提升树算法（对于平方损失函数来说就是不停地拟合残差）看李航书P148以及例8.2
- 当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，需要使用==梯度提升算法==相当于不停地拟合残差的近似值



## GBDT（梯度提升树）

- 推荐博客 
  - https://blog.csdn.net/w28971023/article/details/8240756
  - http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/ 基本把陈天奇论文的主体翻译了一下，先看这个在读论文会好理解一些



## XGBoost

- 推荐博客 https://www.xianjichina.com/news/details_67971.html



## LightGBM

