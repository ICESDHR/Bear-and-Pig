[TOC]

# 集成学习

集成学习分为两类：

- 个体学习器间存在强依赖关系、必须串行生成序列化方法。典型代表是Boosting
- 个体学习器间不存在强依赖关系、可同时生成的并行化方法。典型代表是Bagging和随机森林

## Boosting（提升方法）

- Boosting是一族可将弱学习器提升为强学习器的算法
- 对提升方法来说有两个问题需要回答：
  1. 在每一轮如何改变训练数据的权值或概率分布
  2. 如何将弱分类器组合成一个强分类器
- Boosting算法的“重赋权法”和“重采样法”看西瓜书P177
- ==重启动==看西瓜书P177
- 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。



### AdaBoost算法（李航书P138，P140例子很好懂推荐看）

- AdaBoost算法的特点：
  1. 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用
  2. 利用基本分类器的线性组合构建最终的分类器
- 标准AdaBoost只适用于二分类任务，Bagging能不经修改地用于多分类、回归等任务



### 提升树模型

以决策树为基函数的提升方法称为提升树（boosting tree）

#### 提升树算法

- 对于二分类问题，提升树算法只需将AdaBoost算法中的基分类器限制为二分类树即可
- 回归问题的提升树算法（对于平方损失函数来说就是不停地拟合残差）看李航书P148以及例8.2
- 当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，需要使用==梯度提升算法==相当于不停地拟合残差的近似值

#### GBDT（梯度提升树）

- 推荐博客 
  - https://blog.csdn.net/w28971023/article/details/8240756
  - http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/ 基本把陈天奇论文的主体翻译了一下，先看这个在读论文会好理解一些

#### XGBoost

- 推荐博客 https://www.xianjichina.com/news/details_67971.html

#### LightGBM

- https://blog.csdn.net/qq_33638791/article/details/78972853
- http://lightgbm.apachecn.org/cn/latest/Installation-Guide.html#osx



## Bagging

- Bagging是并行式集成学习方法最著名的代表，它直接基于西瓜书P27介绍的==自助采样法（bootstrap sampling）==
- 自助采样过程给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2％的样本，剩下约36.8％的样本可用作验证集来对泛化性能进行“包外估计”
- 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树。神经网络等易受样本扰动的学习器上效用更为明显



### 随机森林

- 随机森林是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后在从这个子集中选择一个最有属性用于划分。
- 随机森林的训练效率常==优于==Bagging





## 模型融合（stacking）

https://zhuanlan.zhihu.com/p/25836678

https://blog.csdn.net/wstcjf/article/details/77989963 详解stacking过程

https://blog.csdn.net/qq_18916311/article/details/78557722 这个是最好的 感觉

https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/ 如何在 Kaggle 首战中进入前 10%

https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python 泰坦尼克的例子 用了stacking

![img](https://img-blog.csdn.net/20171117094740886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTg5MTYzMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)



# 卷积神经网络(CNN)

- channel表示色彩通道

- 一个卷积神经网络主要由以下5种结构组成：
  - 输入层。
  - 卷积层。卷积层试图将神经网络中的每一块进行更加深入地分析从而得到抽象程度更高的特征。一般来说卷积层处理过得节点矩阵会变得更深。
  - 池化层。池化层神经网络不会改变三维矩阵的深度，但是它可以缩小矩阵的大小。

- 深度学习防止过拟合方法

  https://blog.csdn.net/NoviceRoad/article/details/75734048



# 归一化

- 基于树的方法不需要进行特征的归一化。例如随机森林，bagging与boosting等方法。如果是基于参数的模型或者基于距离的模型，因为需要对参数或者距离进行计算，都需要进行归一化。